{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# Alt Architecture - Noisy Subfeatures! A different seed!\n",
    "\n",
    "How does our NN work with noisy subfeatures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, load_intermediate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"./data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"./data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"./data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"./data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (50000, 28, 28)\n",
      "Val images shape: (10000, 28, 28)\n",
      "Test images shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd06cc-fd16-4933-be26-030360e3a363",
   "metadata": {},
   "source": [
    "## Loading Noisy Subfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1158523f-edd9-4a33-a5f3-150d98d7f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features = load_intermediate_labels(\"min_sub_feature_dict_random_noise_v2.pkl\")\n",
    "sub_features = load_intermediate_labels(\"sub_feature_dict_random_noise_v2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f56082e-cf3b-4306-a6da-e4495d31ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features = [torch.tensor(sf, dtype=torch.float32) for sf in min_sub_features]\n",
    "sub_features = [torch.tensor(sf, dtype=torch.float32) for sf in sub_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18aa3002-845f-4dbf-b5b4-04ce5788caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features_tensor = torch.stack(min_sub_features).to(device)\n",
    "sub_features_tensor = torch.stack(sub_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b48d03-7d6a-4e95-b1ae-2a07f55eed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200000, 28, 28]), torch.Size([100000, 28, 28]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sub_features_tensor.shape, sub_features_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517abc1-b197-47cc-b71a-921b059ba56d",
   "metadata": {},
   "source": [
    "# Our Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We keep our two hidden layers at image size to be able to calculate a local loss to push those layers to learn human recognizable structures. However, for the example below, we don't calculate intermediate loss, as we need a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        layer_size_by_pixels = 28 * 28\n",
    "        \n",
    "        self.first_layer = nn.ModuleList([nn.Linear(layer_size_by_pixels, layer_size_by_pixels) for _ in range(4)])\n",
    "        self.second_layer = nn.ModuleList([nn.Linear(layer_size_by_pixels, layer_size_by_pixels) for _ in range(2)])\n",
    "        self.classification_layer = nn.Linear(layer_size_by_pixels, 10)\n",
    "        self.activation_function = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        first_layer_outputs = []\n",
    "        for layer in self.first_layer:\n",
    "            out = layer(x)\n",
    "            act = self.activation_function(out)\n",
    "            first_layer_outputs.append(act)\n",
    "        \n",
    "        second_layer_outputs = []\n",
    "        for i in range(2):\n",
    "            combined_input = first_layer_outputs[2*i] + first_layer_outputs[2*i + 1]\n",
    "            out = self.second_layer[i](combined_input)\n",
    "            act = self.activation_function(out)\n",
    "            second_layer_outputs.append(act)\n",
    "        \n",
    "        # Combine second_layer_outputs for classification\n",
    "        combined_act = sum(second_layer_outputs)\n",
    "        \n",
    "        classification_out = self.classification_layer(combined_act)\n",
    "        \n",
    "        # Return classification_out and all activations\n",
    "        return classification_out, first_layer_outputs, second_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "sub_feature_loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights (to compare below): [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model weights (to compare below): {model.first_layer[0].weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First set of weights: [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n",
      "Second set of weights: [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n",
      "Are the two sets equal: [ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.first_layer[0].weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.first_layer[0].weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx].copy()).float(),\n",
    "            torch.tensor(self.labels[idx].copy(), dtype=torch.long),\n",
    "            idx\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 35.6161\n",
      "  Val Loss: 3.9140 | Val Acc: 87.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "  Train Loss: 26.7343\n",
      "  Val Loss: 3.1669 | Val Acc: 92.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "  Train Loss: 23.9844\n",
      "  Val Loss: 2.6984 | Val Acc: 93.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "  Train Loss: 22.8611\n",
      "  Val Loss: 2.8131 | Val Acc: 93.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "  Train Loss: 22.2346\n",
      "  Val Loss: 1.6371 | Val Acc: 96.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "  Train Loss: 21.6900\n",
      "  Val Loss: 3.2076 | Val Acc: 94.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "  Train Loss: 21.8069\n",
      "  Val Loss: 2.0287 | Val Acc: 96.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "  Train Loss: 21.4349\n",
      "  Val Loss: 3.5533 | Val Acc: 95.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "  Train Loss: 21.4160\n",
      "  Val Loss: 2.9573 | Val Acc: 96.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "  Train Loss: 21.3483\n",
      "  Val Loss: 5.1518 | Val Acc: 94.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 21.0821\n",
      "  Val Loss: 3.0894 | Val Acc: 96.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "  Train Loss: 20.9536\n",
      "  Val Loss: 3.5029 | Val Acc: 96.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "  Train Loss: 21.6191\n",
      "  Val Loss: 3.7426 | Val Acc: 96.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "  Train Loss: 21.2365\n",
      "  Val Loss: 4.7753 | Val Acc: 96.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "  Train Loss: 21.2436\n",
      "  Val Loss: 4.4370 | Val Acc: 96.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "  Train Loss: 21.2913\n",
      "  Val Loss: 3.3201 | Val Acc: 97.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "  Train Loss: 21.3894\n",
      "  Val Loss: 4.9889 | Val Acc: 96.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "  Train Loss: 21.3101\n",
      "  Val Loss: 4.6470 | Val Acc: 96.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "  Train Loss: 21.0793\n",
      "  Val Loss: 4.7129 | Val Acc: 96.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "  Train Loss: 20.7361\n",
      "  Val Loss: 5.5812 | Val Acc: 96.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "loss_factor = 0.001\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # set the model to training mode - this is currently a no-op\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "    for batch in train_bar:\n",
    "        # deconstruct batch items\n",
    "        images, labels, indices = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # Compute subfeature indices (keep on CPU for tensor indexing)\n",
    "        first_layer_indices = torch.cat([4 * idx + torch.arange(4) for idx in indices])\n",
    "        second_layer_indices = torch.cat([2 * idx + torch.arange(2) for idx in indices])\n",
    "    \n",
    "        # Extract subfeatures and reshape\n",
    "        first_layer_labels = min_sub_features_tensor[first_layer_indices].view(batch_size, 4, 784)\n",
    "        second_layer_labels = sub_features_tensor[second_layer_indices].view(batch_size, 2, 784)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, first_layer_outputs, second_layer_outputs = model(images)\n",
    "        \n",
    "        # --- Loss and Backprop ---\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # subfeature loss\n",
    "        first_loss = sum(sub_feature_loss_fn(out, first_layer_labels[:, i, :]) for i, out in enumerate(first_layer_outputs))\n",
    "        second_loss = sum(sub_feature_loss_fn(out, second_layer_labels[:, i, :]) for i, out in enumerate(second_layer_outputs))\n",
    "\n",
    "        # classification loss\n",
    "        classification_loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "        # total loss\n",
    "        total_loss = loss_factor * (first_loss + second_loss) + classification_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress\n",
    "        train_loss += total_loss.item()\n",
    "        train_bar.set_postfix(loss=classification_loss.item())\n",
    "\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            # deconstruct\n",
    "            images, labels, _  = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            classification_out, _, _ = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # epoch stats\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3ea2-fd60-4e54-8942-33172e4de452",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43901c32-73a7-4236-876a-fd802d9a0f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 294.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 96.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "    for batch in test_bar:\n",
    "        images, labels, _ = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(images)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717fd9a-e8fc-4780-a44a-238a0b56991d",
   "metadata": {},
   "source": [
    "So noise can vary how performance goes... I wonder if we can hit a really bad state. By symmetry, I wonder if we can hit a really good state as well (or perhaps that was our first notebook on noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c324f81-daec-4475-9033-10ef337e06e7",
   "metadata": {},
   "source": [
    "# Exploring the Resulting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec20844-c10d-44ba-bb98-2d5f3c1fdfe4",
   "metadata": {},
   "source": [
    "## Visualizing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b4a8c-0d00-4f25-81c2-c03d61b36212",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_w = np.abs(model.classification_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "vertical_w = np.abs(model.vertical_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "horizontal_w = np.abs(model.horizontal_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 5))\n",
    "\n",
    "visualize_image(horizontal_w, \"Horizontal Layer Weights\", ax=axes[0])\n",
    "visualize_image(vertical_w, \"Vertical Layer Weights\", ax=axes[1])\n",
    "visualize_image(classification_w, \"Classification Layer Weights\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a190a-d3bb-4d84-8124-ea9f6d5f6e10",
   "metadata": {},
   "source": [
    "# Visualizing Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98175107-033e-43be-a43b-6a30ec12f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_examples = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_images[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy()).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, first_act_layer, second_act_layer = model(img_tensor)\n",
    "        \n",
    "        first_act_imgs  = [act.clone().reshape(28, 28).detach().cpu().numpy() for act in first_act_layer]\n",
    "        second_act_imgs = [act.clone().reshape(28, 28).detach().cpu().numpy() for act in second_act_layer]\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        eval_examples.append((label, img, out, first_act_imgs, second_act_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c8f1-1b0b-43da-8672-2a6e99c8fb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, first_act_imgs, second_act_imgs in eval_examples:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\")\n",
    "    for img in first_act_imgs:\n",
    "        visualize_image(img, \"First Layer Activations\")\n",
    "\n",
    "    for img in second_act_imgs:\n",
    "        visualize_image(img, \"Second Layer Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088160a3-3fd3-4fcf-8c74-f0ca4d6bb1a8",
   "metadata": {},
   "source": [
    "# Different Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25df7e0-a134-4f55-9b09-556d00635870",
   "metadata": {},
   "source": [
    "## Horizontal Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79301b-04ae-46bf-8227-da54576676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "h_edge_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_horizontal_image_labels[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        h_edge_inputs.append((label, img.copy().reshape(28, 28), out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2db2f-72ad-4a5c-9301-7d24ce5bf4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in h_edge_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5816bd-a1ca-48d0-9b7d-6a014501de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        if len(horizontal_labels) == 128:\n",
    "            h_imgs = horizontal_labels.reshape(128, 28, 28)\n",
    "        else:\n",
    "            h_imgs = horizontal_labels.reshape(16, 28, 28)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(h_imgs)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d289469-a61e-40ba-8892-3422f3f0c063",
   "metadata": {},
   "source": [
    "## Vertical Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62c544-1f0f-4de2-ba71-a7a8d9304063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "v_edge_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_vertical_image_labels[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        v_edge_inputs.append((label, img.copy().reshape(28, 28), out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace73b9-4edd-41c2-853c-0be864db4a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in v_edge_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718757a8-451c-4fea-b923-2b5628b122ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        if len(horizontal_labels) == 128:\n",
    "            v_imgs = vertical_labels.reshape(128, 28, 28)\n",
    "        else:\n",
    "            v_imgs = vertical_labels.reshape(16, 28, 28)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(v_imgs)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfbcfd-16fa-4209-90b9-537a0439515f",
   "metadata": {},
   "source": [
    "There might be something to vertical edges being more important features than horizontal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeeab6-27af-431c-ba51-de847761a24d",
   "metadata": {},
   "source": [
    "## Random Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e916d629-0ea5-4e52-90f7-80d73d0128f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 28, 28\n",
    "noisy_images = [np.random.randint(0, 256, (height, width), dtype=np.uint8) for _ in range(0, 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f291e3df-effe-4d1b-ac9b-dc050f0634c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noisy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb22e937-3a73-4707-b698-653181e550d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(noisy_images[0], \"Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e8cc9-2d50-41bf-848c-343865faeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "noisy_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for img in noisy_images[:10]:\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = \"Noise\"\n",
    "        noisy_inputs.append((label, img, out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288fc23-5e83-457c-996b-7d481fc5c70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in noisy_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0712b455-e881-4784-b71c-b8957e59c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 12.50%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "answer_dict = {}\n",
    "activation_dict = {}\n",
    "with torch.no_grad():\n",
    "    for img in noisy_images:\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(img_tensor)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "\n",
    "        for i, act in enumerate(classification_out[0]):\n",
    "            if i in activation_dict:\n",
    "                activation_dict[i] += act.item()\n",
    "            else:\n",
    "                activation_dict[i] = act.item()\n",
    "\n",
    "        if predicted.item() in answer_dict:\n",
    "            answer_dict[predicted.item()] += 1\n",
    "        else:\n",
    "            answer_dict[predicted.item()] = 1\n",
    "        \n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f129990-5e26-4c0b-9e60-7fe43f1bf68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 10000}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37d27a5e-6908-4dce-b917-4f754ffc1497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 15621587.18),\n",
       " (5, 4615761.2),\n",
       " (8, 4582521.08),\n",
       " (9, 2807313.61),\n",
       " (0, -3416229.42),\n",
       " (7, -5312476.95),\n",
       " (2, -5325467.12),\n",
       " (1, -7644672.79),\n",
       " (4, -8437730.41),\n",
       " (6, -13256672.81)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(key, round(value, 2)) for key, value in activation_dict.items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b5408-b96b-421a-9bf3-be86c9f088de",
   "metadata": {},
   "source": [
    "Honestly, not a terrible distribution, but it seems our low loss factor is \"best\" in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "478f4b16-d7af-41ad-8e7f-8e72ee1afedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -3416229.4233159497,\n",
       " 1: -7644672.793395996,\n",
       " 2: -5325467.1234436035,\n",
       " 3: 15621587.182861328,\n",
       " 4: -8437730.407745361,\n",
       " 5: 4615761.199920654,\n",
       " 6: -13256672.811096191,\n",
       " 7: -5312476.954317093,\n",
       " 8: 4582521.079285622,\n",
       " 9: 2807313.6075710654}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e553352-62e8-4106-86e0-b277147b7647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = [str(x) for x in range(0, 10)]\n",
    "x = np.array([int(k) for k in activation_dict.keys()])  # positions for the categories\n",
    "y = np.array([int(v) for v in activation_dict.values()])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the first set of bars\n",
    "bars1 = ax.bar(x, y)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Digit Classes')\n",
    "ax.set_ylabel('Summed Activations')\n",
    "ax.set_title('Summed Activations vs. Digit Class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466336ff-07eb-445c-9902-87dd38ae019e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
