{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# Alt Architecture w/Garbage Intermediate Nudging\n",
    "\n",
    "Does the network perform worse when we put arbitrary noise as a random feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, generate_intermediate_edge_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"./data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"./data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"./data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"./data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca05d5-d791-4fbb-9736-b52a5eae1409",
   "metadata": {},
   "source": [
    "## Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f14c-5f30-4237-933c-55a6f254a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = train_images[0]\n",
    "sample_label = train_labels[0]\n",
    "visualize_image(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f083a3-1e2c-433a-b03d-51c00adb105d",
   "metadata": {},
   "source": [
    "## Generating the Noisy Intermediate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc3b6bd-6bf2-411d-89e3-55fbc8ff8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517abc1-b197-47cc-b71a-921b059ba56d",
   "metadata": {},
   "source": [
    "# Our Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We keep our two hidden layers at image size to be able to calculate a local loss to push those layers to learn human recognizable structures. However, for the example below, we don't calculate intermediate loss, as we need a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_size_by_pixels = 28*28\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # define layers separately to have access to each\n",
    "        self.horizontal_layer = nn.Linear(layer_size_by_pixels, layer_size_by_pixels)\n",
    "        self.vertical_layer = nn.Linear(layer_size_by_pixels, layer_size_by_pixels)\n",
    "        self.classification_layer = nn.Linear(layer_size_by_pixels, 10)\n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # horizontal layer\n",
    "        horizontal_out = self.horizontal_layer(x)\n",
    "        horizontal_act = self.activation_function(horizontal_out)\n",
    "\n",
    "        # vertical layer connected to horizontal layer\n",
    "        vertical_out = self.vertical_layer(x)\n",
    "        vertical_act = self.activation_function(vertical_out)\n",
    "\n",
    "        combined_act = (vertical_act + horizontal_act) / 2\n",
    "\n",
    "        # coupling layer\n",
    "        classification_out = self.classification_layer(combined_act)\n",
    "        \n",
    "        return classification_out, vertical_act, horizontal_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "horizontal_loss_fn = nn.MSELoss()\n",
    "vertical_loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model weights (to compare below): {model.horizontal_layer.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.horizontal_layer.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.horizontal_layer.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels, horizontal_edges, vertical_edges):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.horizontal_edges = horizontal_edges\n",
    "        self.vertical_edges = vertical_edges\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx].copy()).float(),\n",
    "            torch.tensor(self.labels[idx].copy(), dtype=torch.long),\n",
    "            torch.from_numpy(self.horizontal_edges[idx].copy()).float(),\n",
    "            torch.from_numpy(self.vertical_edges[idx].copy()).float(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels, train_horizontal_image_labels, train_vertical_image_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels, val_horizontal_image_labels, val_vertical_image_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels, test_horizontal_image_labels, test_vertical_image_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "loss_factor = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # set the model to training mode - this is currently a no-op\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # visualize weights at each layer during training\n",
    "    classification_w = np.abs(model.classification_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "    vertical_w = np.abs(model.vertical_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "    horizontal_w = np.abs(model.horizontal_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9, 5))\n",
    "    \n",
    "    visualize_image(horizontal_w, \"Horizontal Layer Weights\", ax=axes[0])\n",
    "    visualize_image(vertical_w, \"Vertical Layer Weights\", ax=axes[1])\n",
    "    visualize_image(classification_w, \"Classification Layer Weights\", ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "    for batch in train_bar:\n",
    "        # deconstruct batch items\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, vertical_act, horizontal_act = model(images)\n",
    "        \n",
    "        # --- Loss and Backprop ---\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # vertical loss\n",
    "        vertical_loss = vertical_loss_fn(vertical_act, vertical_labels)\n",
    "\n",
    "        # horizontal loss\n",
    "        horizontal_loss = horizontal_loss_fn(horizontal_act, horizontal_labels)\n",
    "\n",
    "        # classification loss\n",
    "        classification_loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "        # total loss\n",
    "        total_loss = loss_factor * (vertical_loss + horizontal_loss) + classification_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress\n",
    "        train_loss += total_loss.item()\n",
    "        train_bar.set_postfix(loss=classification_loss.item())\n",
    "\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            # deconstruct\n",
    "            images, labels, _, _ = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            classification_out, _, _ = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # epoch stats\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3ea2-fd60-4e54-8942-33172e4de452",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43901c32-73a7-4236-876a-fd802d9a0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "    for batch in test_bar:\n",
    "        images, labels, _, _ = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(images)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c324f81-daec-4475-9033-10ef337e06e7",
   "metadata": {},
   "source": [
    "# Exploring the Resulting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec20844-c10d-44ba-bb98-2d5f3c1fdfe4",
   "metadata": {},
   "source": [
    "## Visualizing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b4a8c-0d00-4f25-81c2-c03d61b36212",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_w = np.abs(model.classification_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "vertical_w = np.abs(model.vertical_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "horizontal_w = np.abs(model.horizontal_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 5))\n",
    "\n",
    "visualize_image(horizontal_w, \"Horizontal Layer Weights\", ax=axes[0])\n",
    "visualize_image(vertical_w, \"Vertical Layer Weights\", ax=axes[1])\n",
    "visualize_image(classification_w, \"Classification Layer Weights\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a190a-d3bb-4d84-8124-ea9f6d5f6e10",
   "metadata": {},
   "source": [
    "# Visualizing Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98175107-033e-43be-a43b-6a30ec12f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_examples = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_images[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy()).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        eval_examples.append((label, img, out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c8f1-1b0b-43da-8672-2a6e99c8fb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in eval_examples:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088160a3-3fd3-4fcf-8c74-f0ca4d6bb1a8",
   "metadata": {},
   "source": [
    "# Different Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25df7e0-a134-4f55-9b09-556d00635870",
   "metadata": {},
   "source": [
    "## Horizontal Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79301b-04ae-46bf-8227-da54576676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "h_edge_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_horizontal_image_labels[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        h_edge_inputs.append((label, img.copy().reshape(28, 28), out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2db2f-72ad-4a5c-9301-7d24ce5bf4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in h_edge_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5816bd-a1ca-48d0-9b7d-6a014501de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        if len(horizontal_labels) == 128:\n",
    "            h_imgs = horizontal_labels.reshape(128, 28, 28)\n",
    "        else:\n",
    "            h_imgs = horizontal_labels.reshape(16, 28, 28)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(h_imgs)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d289469-a61e-40ba-8892-3422f3f0c063",
   "metadata": {},
   "source": [
    "## Vertical Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62c544-1f0f-4de2-ba71-a7a8d9304063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "v_edge_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_vertical_image_labels[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        v_edge_inputs.append((label, img.copy().reshape(28, 28), out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace73b9-4edd-41c2-853c-0be864db4a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in v_edge_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718757a8-451c-4fea-b923-2b5628b122ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        if len(horizontal_labels) == 128:\n",
    "            v_imgs = vertical_labels.reshape(128, 28, 28)\n",
    "        else:\n",
    "            v_imgs = vertical_labels.reshape(16, 28, 28)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(v_imgs)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfbcfd-16fa-4209-90b9-537a0439515f",
   "metadata": {},
   "source": [
    "There might be something to vertical edges being more important features than horizontal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeeab6-27af-431c-ba51-de847761a24d",
   "metadata": {},
   "source": [
    "## Random Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916d629-0ea5-4e52-90f7-80d73d0128f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 28, 28\n",
    "noisy_images = [np.random.randint(0, 256, (height, width), dtype=np.uint8) for _ in range(0, 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291e3df-effe-4d1b-ac9b-dc050f0634c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noisy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22e937-3a73-4707-b698-653181e550d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(noisy_images[0], \"Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e8cc9-2d50-41bf-848c-343865faeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "noisy_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for img in noisy_images[:10]:\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = \"Noise\"\n",
    "        noisy_inputs.append((label, img, out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288fc23-5e83-457c-996b-7d481fc5c70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in noisy_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712b455-e881-4784-b71c-b8957e59c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "answer_dict = {}\n",
    "activation_dict = {}\n",
    "with torch.no_grad():\n",
    "    for img in noisy_images:\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(img_tensor)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "\n",
    "        for i, act in enumerate(classification_out[0]):\n",
    "            if i in activation_dict:\n",
    "                activation_dict[i] += act.item()\n",
    "            else:\n",
    "                activation_dict[i] = act.item()\n",
    "\n",
    "        if predicted.item() in answer_dict:\n",
    "            answer_dict[predicted.item()] += 1\n",
    "        else:\n",
    "            answer_dict[predicted.item()] = 1\n",
    "        \n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f129990-5e26-4c0b-9e60-7fe43f1bf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d27a5e-6908-4dce-b917-4f754ffc1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(key, round(value, 2)) for key, value in activation_dict.items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b5408-b96b-421a-9bf3-be86c9f088de",
   "metadata": {},
   "source": [
    "Honestly, not a terrible distribution, but it seems our low loss factor is \"best\" in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f4b16-d7af-41ad-8e7f-8e72ee1afedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e553352-62e8-4106-86e0-b277147b7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [str(x) for x in range(0, 10)]\n",
    "x = np.array([int(k) for k in activation_dict.keys()])  # positions for the categories\n",
    "y = np.array([int(v) for v in activation_dict.values()])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the first set of bars\n",
    "bars1 = ax.bar(x, y)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Digit Classes')\n",
    "ax.set_ylabel('Summed Activations')\n",
    "ax.set_title('Summed Activations vs. Digit Class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466336ff-07eb-445c-9902-87dd38ae019e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
