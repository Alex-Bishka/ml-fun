{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1a9d52-b18f-45fe-bdd3-bfe00363c8cd",
   "metadata": {},
   "source": [
    "# Sub Labeling Data\n",
    "\n",
    "Instead of getting average average the numbers, then getting their subfeatures, let's just get the subfeature for each number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb983cb-7930-4236-88c2-4593339040b5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a6a67d-91d0-4f15-a5d9-498b77dbb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, generate_intermediate_edge_labels, horizontal_kernel, vertical_kernel, \\\n",
    "    load_intermediate_labels, generate_intermediate_curve_labels, save_intermediate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b6168-e364-4248-a6d4-3355286ada7d",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c7361f-d520-4e5c-8a13-fb1ef92fb592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5dc49-2a7c-47ba-92fb-8e1d107b04dc",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2dfe061-7b24-4543-9d4b-01262ac9eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"./data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"./data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"./data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"./data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef986e2-4918-43c4-9184-bc5e1cb9e0f7",
   "metadata": {},
   "source": [
    "## Generating Additional Sub Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba0d642-8df9-4668-adf7-6950240d483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_7 = train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a533f6-894c-4a95-a7d3-5ed8f9631d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(ex_7, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a3a58b-55f0-4be9-9af0-0323d7ffe6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_chunks_in_position(image, chunk_size, min_intensity_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Extract chunks and visualize each against a black background in its original position.\n",
    "    Only includes chunks with sufficient brightness.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: 2D numpy array (28x28 for MNIST)\n",
    "    - chunk_size: size of each chunk\n",
    "    - min_intensity_threshold: minimum average intensity (0-1) to keep a chunk\n",
    "    \"\"\"\n",
    "    h, w = image.shape\n",
    "    n_chunks_h = h // chunk_size\n",
    "    n_chunks_w = w // chunk_size\n",
    "    \n",
    "    # Calculate number of chunks to show (potentially fewer after filtering)\n",
    "    # We'll create a grid layout with a bit of extra space\n",
    "    fig, axes = plt.subplots(n_chunks_h, n_chunks_w, figsize=(10, 10))\n",
    "    \n",
    "    # Handle the case of a single row or column\n",
    "    if n_chunks_h == 1:\n",
    "        axes = np.array([axes])\n",
    "    if n_chunks_w == 1:\n",
    "        axes = np.array([[ax] for ax in axes])\n",
    "\n",
    "    chunks = []\n",
    "    # Process each chunk position\n",
    "    for i in range(n_chunks_h):\n",
    "        for j in range(n_chunks_w):\n",
    "            start_h = i * chunk_size\n",
    "            start_w = j * chunk_size\n",
    "            \n",
    "            # Extract the chunk\n",
    "            chunk = image[start_h:start_h+chunk_size, start_w:start_w+chunk_size]\n",
    "            \n",
    "            # Calculate average intensity (assuming values are 0-255)\n",
    "            avg_intensity = np.mean(chunk) / 255.0\n",
    "            \n",
    "            # Create a blank image\n",
    "            chunk_in_position = np.zeros_like(image)\n",
    "            \n",
    "            # Only place the chunk if it has enough brightness\n",
    "            if avg_intensity > min_intensity_threshold:\n",
    "                # Place the chunk in its original position\n",
    "                chunk_in_position[start_h:start_h+chunk_size, start_w:start_w+chunk_size] = chunk\n",
    "                axes[i, j].set_title(f\"Avg: {avg_intensity:.2f}\")\n",
    "                chunks.append(chunk_in_position)\n",
    "            \n",
    "                # Display the chunk in its original position against a black background\n",
    "                axes[i, j].imshow(chunk_in_position, cmap='gray')\n",
    "                axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082adf1-08ed-46ca-bad0-950658055019",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8190e39-9918-47ed-95cc-8b62455df510",
   "metadata": {},
   "source": [
    "Some issues with chunking to get import sub features and then build up:\n",
    "- unequal amount of important subfeatures between numbers\n",
    "- how do we handle this for each layer?\n",
    "  - do we need to just have each of the 16 regions above to correspond to a \"feature\"? then condense it to 4 for the next layer?\n",
    " \n",
    "**Idea:** find the n unique subfeatures of digits. Then group these into n/k unique subfeatures, and so on, until we hit our classifier. Each node/(set of) filter(s) should correspond to a unique subfeature. As we go across layers, these subfeatures build upon each other to construct our digit.\n",
    "\n",
    "To find the n unique subfeatures, we use our chunking technique above. After finding chunks for each digit, we also use similarity search s.t. we can combine important features across digits (i.e. subfeatures that form loops in a 0 may also coincide with the subfeatures to form a loop in an 8). \n",
    "\n",
    "The tricky part here is space... do we take that into account when creating these subfeatures? e.g. horizontal line subfeatures might be different at different parts of a digit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f294485-aa99-435e-8785-6569d119673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfeatures(image, chunk_size, min_intensity_threshold=0.1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    h, w = image.shape\n",
    "    n_chunks_h = h // chunk_size\n",
    "    n_chunks_w = w // chunk_size\n",
    "\n",
    "    chunks = []\n",
    "    # Process each chunk position\n",
    "    for i in range(n_chunks_h):\n",
    "        for j in range(n_chunks_w):\n",
    "            start_h = i * chunk_size\n",
    "            start_w = j * chunk_size\n",
    "            \n",
    "            # Extract the chunk\n",
    "            chunk = image[start_h:start_h+chunk_size, start_w:start_w+chunk_size]\n",
    "            \n",
    "            # Calculate average intensity (assuming values are 0-255)\n",
    "            avg_intensity = np.mean(chunk) / 255.0\n",
    "            \n",
    "            # Create a blank image\n",
    "            chunk_in_position = np.zeros_like(image)\n",
    "            \n",
    "            # Only place the chunk if it has enough brightness\n",
    "            if avg_intensity > min_intensity_threshold:\n",
    "                # Place the chunk in its original position\n",
    "                chunk_in_position[start_h:start_h+chunk_size, start_w:start_w+chunk_size] = chunk\n",
    "                chunks.append(chunk_in_position)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b6e46-4e2f-4b30-ad38-95efec4a5194",
   "metadata": {},
   "source": [
    "**New approach:** it's too difficult to extract subfeatures comparing across every single image in the dataset. So, instead, we'll get the average digit, then get it's important subfeatures, and then get the unique subfeatures across the subfeatures of each average digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d22d13f-1f2e-4db1-8435-3540a45dd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features = []\n",
    "sub_features = []\n",
    "for img in train_images:\n",
    "    chunks = get_subfeatures(image=img, chunk_size=14, min_intensity_threshold=-10)\n",
    "    if chunks:\n",
    "        min_sub_features.extend(chunks)\n",
    "\n",
    "        sub_f = [chunks[0] + chunks[1], chunks[2] + chunks[3]]\n",
    "        sub_features.extend(sub_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2189ceb0-5799-4c6d-ad7c-fcf300e3b64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 100000, 50000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min_sub_features), len(sub_features), len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daaf19db-1768-48b3-bc7c-3abcad2fd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate label has been saved!\n",
      "Intermediate label has been saved!\n"
     ]
    }
   ],
   "source": [
    "save_intermediate_labels(\"min_sub_feature_dict_v2.pkl\", min_sub_features)\n",
    "save_intermediate_labels(\"sub_feature_dict_v2.pkl\", sub_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45b619-abe1-490e-a598-9285e005235e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
