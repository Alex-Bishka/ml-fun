{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# SAE on Normal NN MNIST\n",
    "\n",
    "First SAE experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa37d0-59c1-4368-aec0-83ccb0ce6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EXPERIMENT_TYPE = \"SAE\"\n",
    "RUN_ID = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c069cc3-458e-4825-9828-c771c7f7370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# assume cwd is project_root/data_loader\n",
    "project_root = Path(os.getcwd()).parent  # go up one level to project_root\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, generate_intermediate_edge_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"../data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"../data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"../data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"../data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca05d5-d791-4fbb-9736-b52a5eae1409",
   "metadata": {},
   "source": [
    "## Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f14c-5f30-4237-933c-55a6f254a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = train_images[0]\n",
    "sample_label = train_labels[0]\n",
    "visualize_image(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e89c1f-7b41-4537-808c-4a38c5453838",
   "metadata": {},
   "source": [
    "# Helper to Plot Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304c16a-372f-4580-bc57-ee319fcaa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing code to visualize weights\n",
    "def plot_weights(model, epoch):\n",
    "    save_path_hidden_one = f'./weights/{EXPERIMENT_TYPE}/{RUN_ID}/hidden_one/hidden_one_weights_{epoch + 1}.png'\n",
    "    save_path_classification = f'./weights/{EXPERIMENT_TYPE}/{RUN_ID}/classification/classification_weights_{epoch + 1}.png'\n",
    "    save_path_hidden_two = f'./weights/{EXPERIMENT_TYPE}/{RUN_ID}/hidden_two/hidden_two_weights_{epoch + 1}.png'\n",
    "\n",
    "    for path in [save_path_hidden_one, save_path_hidden_two, save_path_classification]:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Extract weights from each layer\n",
    "    hidden_one_w = model.hidden_one.weight.detach().cpu().numpy()  # Shape: (16, 784)\n",
    "    hidden_two_w = model.hidden_two.weight.detach().cpu().numpy()  # Shape: (16, 16)\n",
    "    classification_w = model.classification_layer.weight.detach().cpu().numpy()  # Shape: (10, 16)\n",
    "\n",
    "    # Figure 1: Hidden One Weights (4x4 grid of 28x28 images)\n",
    "    fig1, axes1 = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    for i in range(16):\n",
    "        row, col = divmod(i, 4)\n",
    "        neuron_w = np.abs(hidden_one_w[i].reshape(28, 28))  # Reshape to 28x28\n",
    "        axes1[row, col].imshow(neuron_w, cmap='gray')\n",
    "        axes1[row, col].set_title(f\"H1 Neuron {i+1}\")\n",
    "        axes1[row, col].axis('off')\n",
    "    plt.suptitle(f\"Hidden One Weights - Epoch {epoch+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path_hidden_one)\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 2: Hidden Two Weights (4x4 grid of line plots)\n",
    "    # TODO: change this to scatter plot (these should be disrete data points)\n",
    "    fig2, axes2 = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    min_w = hidden_two_w.min()\n",
    "    max_w = hidden_two_w.max()\n",
    "    for i in range(16):\n",
    "        row, col = divmod(i, 4)\n",
    "        axes2[row, col].plot(hidden_two_w[i])\n",
    "        axes2[row, col].set_title(f\"H2 Neuron {i+1}\")\n",
    "        axes2[row, col].set_xlabel(\"From H1 Neuron\")\n",
    "        axes2[row, col].set_ylabel(\"Weight\")\n",
    "        axes2[row, col].set_ylim(min_w, max_w)\n",
    "    plt.suptitle(f\"Hidden Two Weights - Epoch {epoch+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path_hidden_two)\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 3: Classification Weights (2x5 grid of line plots)\n",
    "    fig3, axes3 = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    min_w = classification_w.min()\n",
    "    max_w = classification_w.max()\n",
    "    for j in range(10):\n",
    "        row, col = divmod(j, 5)\n",
    "        axes3[row, col].plot(classification_w[j])\n",
    "        axes3[row, col].set_title(f\"Class {j}\")\n",
    "        axes3[row, col].set_xlabel(\"From H2 Neuron\")\n",
    "        axes3[row, col].set_ylabel(\"Weight\")\n",
    "        axes3[row, col].set_ylim(min_w, max_w)\n",
    "    plt.suptitle(f\"Classification Weights - Epoch {epoch+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path_classification)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "### NN\n",
    "\n",
    "Once again, two hidden layers. 16 nodes each. Same as 3blue1brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_size_by_pixels = 28*28\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # define layers separately to have access to each\n",
    "        self.hidden_one = nn.Linear(layer_size_by_pixels, 16)\n",
    "        self.hidden_two = nn.Linear(16, 16)\n",
    "        self.classification_layer = nn.Linear(16, 10)\n",
    "        \n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # first hidden layer\n",
    "        hidden_one_out = self.hidden_one(x)\n",
    "        hidden_one_act = self.activation_function(hidden_one_out)\n",
    "\n",
    "        # second hidden layer\n",
    "        hidden_two_out = self.hidden_two(hidden_one_act)\n",
    "        hidden_two_act = self.activation_function(hidden_two_out)\n",
    "\n",
    "        # classification layer\n",
    "        classification_out = self.classification_layer(hidden_two_act)\n",
    "        \n",
    "        return classification_out, hidden_one_act, hidden_two_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model weights (to compare below): {model.hidden_one.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44c3cf-602d-451e-b10c-8b7f005858e8",
   "metadata": {},
   "source": [
    "### SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53786553-7c2c-4d0f-96a9-b901297c7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=16, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.activation(self.encoder(x))\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed, encoded\n",
    "    \n",
    "    def loss(self, x, reconstructed, encoded, l1_lambda=0.001):\n",
    "        mse_loss = nn.MSELoss()(reconstructed, x)\n",
    "        l1_loss = l1_lambda * torch.mean(torch.abs(encoded))\n",
    "        return mse_loss + l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba9c08-a5f4-47c2-b82a-3ea1bb8a41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SparseAutoencoder(input_size=16, hidden_size=128).to(device)\n",
    "optimizer_sae = torch.optim.Adam(sae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.hidden_one.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.hidden_one.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx].copy()).float(),\n",
    "            torch.tensor(self.labels[idx].copy(), dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "if device.type.lower() == \"cpu\":\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "num_epochs = 20\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # set the model to training mode - this is currently a no-op\n",
    "    sae.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    total_sae_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "    for batch in train_bar:\n",
    "        # deconstruct batch items\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, hidden_act_one, hidden_act_two = model(images)\n",
    "\n",
    "        # Classification loss and backprop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = classification_loss_fn(classification_out, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += total_loss.item()\n",
    "        train_bar.set_postfix(loss=total_loss.item())\n",
    "\n",
    "        # to prevent backprop on both graphs:\n",
    "        hidden_act_two_detached = hidden_act_two.detach()\n",
    "        \n",
    "        # SAE loss and backprop\n",
    "        optimizer_sae.zero_grad()\n",
    "        reconstructed, encoded = sae(hidden_act_two_detached)\n",
    "        sae_loss = sae.loss(hidden_act_two_detached, reconstructed, encoded)\n",
    "        sae_loss.backward()\n",
    "        optimizer_sae.step()\n",
    "        total_sae_loss += sae_loss.item()        \n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            # deconstruct\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            classification_out, _, _ = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # epoch stats\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_sae_train_loss = sae_loss / len(train_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  SAE Train Loss: {avg_sae_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    model_path = f'./models/{EXPERIMENT_TYPE}/{RUN_ID}/best_model_baseline_{epoch+1}.pth'\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_val_loss = avg_val_loss  # Update loss for reference\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"  Saved model with Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "        best_model = model\n",
    "        \n",
    "    # Optional: Save if accuracy is equal but loss is lower\n",
    "    elif val_accuracy == best_val_acc and avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"  Saved model with same Val Acc: {val_accuracy:.2f}% but lower Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        best_model = model\n",
    "\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    plot_weights(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4aa2ac-a39c-4714-bb36-094a5267a5d5",
   "metadata": {},
   "source": [
    "# Training Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984ce30-b454-418a-8a73-31f60905132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(training_loss, validation_loss):\n",
    "    epochs = np.arange(1, len(validation_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, training_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, validation_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06952990-a3a5-44f3-9325-21f1de91dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(training_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3ea2-fd60-4e54-8942-33172e4de452",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43901c32-73a7-4236-876a-fd802d9a0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "    for batch in test_bar:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = best_model(images)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91d255-58b1-46fe-b90f-e4b15f7a0903",
   "metadata": {},
   "source": [
    "# SAE Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819be06-361c-42a3-90f1-b64f2ae8afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features\n",
    "feature_activations = torch.zeros(128, len(dataset))\n",
    "with torch.no_grad():\n",
    "    for i, (images, _) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        _, _, hidden_two_act = classifier(images)\n",
    "        _, encoded = sae(hidden_two_act)\n",
    "        feature_activations[:, i*1000:(i+1)*1000] = encoded.T.cpu()\n",
    "\n",
    "# Find top activating images for each feature\n",
    "top_k = 10\n",
    "for feature_idx in range(128):\n",
    "    activations = feature_activations[feature_idx]\n",
    "    top_indices = torch.topk(activations, top_k).indices\n",
    "    print(f\"Feature {feature_idx} top {top_k} activations:\")\n",
    "    for idx in top_indices:\n",
    "        image, label = dataset[idx]\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c324f81-daec-4475-9033-10ef337e06e7",
   "metadata": {},
   "source": [
    "# Activation Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21fe82-9ae0-4259-b141-95428b050cdd",
   "metadata": {},
   "source": [
    "## Helper to visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa939e-845d-4c0a-a0d4-2c2fb6c64ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(hidden_one_act, hidden_two_act, classification_out, label, save_plt=False, meta=None):\n",
    "    if meta:\n",
    "        label = f\"{label}_{meta}\"\n",
    "        \n",
    "    # Convert tensors to numpy arrays\n",
    "    hidden_one_act_np = hidden_one_act.cpu().detach().numpy()\n",
    "    hidden_two_act_np = hidden_two_act.cpu().detach().numpy()\n",
    "    classification_act_np = classification_out.cpu().detach().numpy()\n",
    "    \n",
    "    # Squeeze any singleton dimensions (e.g., (1, 16) -> (16,))\n",
    "    hidden_one_act_np = np.squeeze(hidden_one_act_np)\n",
    "    hidden_two_act_np = np.squeeze(hidden_two_act_np)\n",
    "    classification_act_np = np.squeeze(classification_act_np)\n",
    "    \n",
    "    def normalize(acts):\n",
    "        # Handle scalar or single-value arrays\n",
    "        if acts.size == 1:\n",
    "            return np.array([0.5])  # Map to middle of colormap\n",
    "            \n",
    "        # Add epsilon to avoid division by zero\n",
    "        return (acts - acts.min()) / (acts.max() - acts.min() + 1e-8)\n",
    "        \n",
    "    hidden_one_act_norm = normalize(hidden_one_act_np)\n",
    "    hidden_two_act_norm = normalize(hidden_two_act_np)\n",
    "    classification_act_norm = normalize(classification_act_np)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 12))\n",
    "\n",
    "    ax.set_facecolor('#ADD8E6')\n",
    "    fig.patch.set_facecolor('#ADD8E6')\n",
    "    \n",
    "    for i in range(16):  # Hidden Layer 1\n",
    "        ax.add_patch(plt.Circle((1, i), radius=0.5, color=plt.cm.gray(hidden_one_act_norm[i])))\n",
    "        \n",
    "    for i in range(16):  # Hidden Layer 2\n",
    "        ax.add_patch(plt.Circle((2, i), radius=0.5, color=plt.cm.gray(hidden_two_act_norm[i])))\n",
    "        \n",
    "    for i in range(10):  # Output Layer\n",
    "        ax.add_patch(plt.Circle((3, i), radius=0.5, color=plt.cm.gray(classification_act_norm[i])))\n",
    "             \n",
    "    ax.set_ylim(-1, 16)\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_xticks([1, 2, 3])\n",
    "    ax.set_xticklabels(['Hidden 1', 'Hidden 2', 'Output'])\n",
    "    ax.set_yticks(range(16))  # Explicit ticks for 0–15\n",
    "    ax.set_yticklabels(range(16))\n",
    "    ax.set_title(f\"Neural Network Activation Visualization - {label}\")\n",
    "\n",
    "    if save_plt:\n",
    "        path = f\"./avg_activations/{EXPERIMENT_TYPE}/{RUN_ID}/{label}.png\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb75cd-de82-4939-8dac-ace31a8fac45",
   "metadata": {},
   "source": [
    "### Averaging Across All Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97477830-6ddd-45e3-8c4a-9fe44a8579af",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\"hidden_one\", \"hidden_two\", \"classification_out\"]\n",
    "def get_avg_layer(label, layer, act, stor_dict):\n",
    "    if label in stor_dict:\n",
    "        cur = avg_act_num_dict[label][layer]\n",
    "        cur.append(act)\n",
    "        avg_act_num_dict[label][layer] = cur\n",
    "\n",
    "    else:\n",
    "        avg_act_num_dict[label] = {}\n",
    "        avg_act_num_dict[label][layer] = [act]\n",
    "\n",
    "        for l in layers:\n",
    "            if l != layer:\n",
    "                avg_act_num_dict[label][l] = []\n",
    "        \n",
    "    return stor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a0a00-e8e0-4a27-9289-6b6ce4b9d7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_act_num_dict = {}\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_labels)):\n",
    "        image = image = torch.from_numpy(test_images[i]).float().unsqueeze(0)\n",
    "        label = test_labels[i]\n",
    "\n",
    "        # Move to device\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        classification_out, hidden_one_act, hidden_two_act = best_model(image)\n",
    "\n",
    "        # Get predicted class\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        predicted = predicted.item()\n",
    "\n",
    "        layer = \"hidden_one\"\n",
    "        avg_act_num_dict = get_avg_layer(label, layer, hidden_one_act, avg_act_num_dict)\n",
    "\n",
    "        layer = \"hidden_two\"\n",
    "        avg_act_num_dict = get_avg_layer(label, layer, hidden_two_act, avg_act_num_dict)\n",
    "\n",
    "        layer = \"classification_out\"\n",
    "        avg_act_num_dict = get_avg_layer(label, layer, classification_out, avg_act_num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcddb0-69d1-4aa4-adf5-b3530cafafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in avg_act_num_dict.keys():\n",
    "    for layer in avg_act_num_dict[label].keys():\n",
    "        acts = torch.cat(avg_act_num_dict[label][layer], dim=0)\n",
    "        avg_act = torch.mean(acts, dim=0)\n",
    "\n",
    "        avg_act_num_dict[label][layer] = avg_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f321ff-3817-4d02-beda-1d375d8acb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_act_num_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b09ce-74b3-4a56-89cd-0ae053fe0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in avg_act_num_dict.keys():\n",
    "    hidden_one_act = avg_act_num_dict[num][\"hidden_one\"]\n",
    "    hidden_two_act = avg_act_num_dict[num][\"hidden_two\"]\n",
    "    classification_out = avg_act_num_dict[num][\"classification_out\"]\n",
    "\n",
    "    plot_activations(hidden_one_act, hidden_two_act, classification_out, num, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cf796-0d49-438e-a746-e8970614df80",
   "metadata": {},
   "source": [
    "## Seeing Specific Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f30ed-5f26-4cda-a725-6b18e3adff6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        # Get single image and label\n",
    "        image = torch.from_numpy(test_images[i]).float().unsqueeze(0)\n",
    "        label = test_labels[i]\n",
    "\n",
    "        # Move to device\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        classification_out, hidden_one_act, hidden_two_act = best_model(image)\n",
    "\n",
    "        # Get predicted class\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        predicted = predicted.item()\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Image {i+1}: True Label = {label}, Predicted Label = {predicted}\")\n",
    "\n",
    "        plot_activations(hidden_one_act, hidden_two_act, classification_out, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9be0ca-a9c1-4897-888e-a021c4bd77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "image_np = test_images[idx]\n",
    "image_np_filt = np.where(image_np >= 180, image_np, 0)\n",
    "\n",
    "image = torch.from_numpy(image_np).float().unsqueeze(0)\n",
    "image_filt = torch.from_numpy(image_np_filt).float().unsqueeze(0)\n",
    "label = test_labels[idx]\n",
    "\n",
    "visualize_image(image_np, label)\n",
    "visualize_image(image_np_filt, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1bb01-3580-4bc2-a043-aa03cc5d3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image, image_filt]\n",
    "with torch.no_grad():\n",
    "    for i, img in enumerate(images):\n",
    "        # Move to device\n",
    "        img = img.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        classification_out, hidden_one_act, hidden_two_act = best_model(img)\n",
    "        \n",
    "        # Get predicted class\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        predicted = predicted.item()\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Image: True Label = {label}, Predicted Label = {predicted}\")\n",
    "\n",
    "        meta = \"normal\"\n",
    "        if i != 0:\n",
    "            meta = \"filt\"\n",
    "        plot_activations(hidden_one_act, hidden_two_act, classification_out, label, True, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea68ec-a34c-4065-9ea0-f28b995adaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
