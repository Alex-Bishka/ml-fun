{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# Normal NN MNIST\n",
    "\n",
    "I want the sterotypical NN here. We'll use 3blue1brown's design. This will be our baseline before diving into adding all of the SAE/transcoder/deep supervision fluff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10aa37d0-59c1-4368-aec0-83ccb0ce6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_TYPE = \"standard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c069cc3-458e-4825-9828-c771c7f7370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# assume cwd is project_root/data_loader\n",
    "project_root = Path(os.getcwd()).parent  # go up one level to project_root\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, generate_intermediate_edge_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"../data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"../data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"../data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"../data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (50000, 28, 28)\n",
      "Val images shape: (10000, 28, 28)\n",
      "Test images shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca05d5-d791-4fbb-9736-b52a5eae1409",
   "metadata": {},
   "source": [
    "## Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a6f14c-5f30-4237-933c-55a6f254a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = train_images[0]\n",
    "sample_label = train_labels[0]\n",
    "visualize_image(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e89c1f-7b41-4537-808c-4a38c5453838",
   "metadata": {},
   "source": [
    "# Helper to Plot Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e304c16a-372f-4580-bc57-ee319fcaa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing code to visualize weights\n",
    "def plot_weights(model, epoch):\n",
    "    save_path_hidden_one = f'./weights/{EXPERIMENT_TYPE}/hidden_one/hidden_one_weights_{epoch + 1}.png'\n",
    "    save_path_classification = f'./weights/{EXPERIMENT_TYPE}/classification/classification_weights_{epoch + 1}.png'\n",
    "    save_path_hidden_two = f'./weights/{EXPERIMENT_TYPE}/hidden_two/hidden_two_weights_{epoch + 1}.png'\n",
    "\n",
    "    for path in [save_path_hidden_one, save_path_hidden_two, save_path_classification]:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Extract weights from each layer\n",
    "    hidden_one_w = model.hidden_one.weight.detach().cpu().numpy()  # Shape: (16, 784)\n",
    "    hidden_two_w = model.hidden_two.weight.detach().cpu().numpy()  # Shape: (16, 16)\n",
    "    classification_w = model.classification_layer.weight.detach().cpu().numpy()  # Shape: (10, 16)\n",
    "\n",
    "    # Figure 1: Hidden One Weights (4x4 grid of 28x28 images)\n",
    "    fig1, axes1 = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    for i in range(16):\n",
    "        row, col = divmod(i, 4)\n",
    "        neuron_w = np.abs(hidden_one_w[i].reshape(28, 28))  # Reshape to 28x28\n",
    "        axes1[row, col].imshow(neuron_w, cmap='gray')\n",
    "        axes1[row, col].set_title(f\"H1 Neuron {i+1}\")\n",
    "        axes1[row, col].axis('off')\n",
    "    plt.suptitle(f\"Hidden One Weights - Epoch {epoch+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path_hidden_one)\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 2: Hidden Two Weights (4x4 grid of line plots)\n",
    "    # TODO: change this to scatter plot (these should be disrete data points)\n",
    "    fig2, axes2 = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    min_w = hidden_two_w.min()\n",
    "    max_w = hidden_two_w.max()\n",
    "    for i in range(16):\n",
    "        row, col = divmod(i, 4)\n",
    "        axes2[row, col].plot(hidden_two_w[i])\n",
    "        axes2[row, col].set_title(f\"H2 Neuron {i+1}\")\n",
    "        axes2[row, col].set_xlabel(\"From H1 Neuron\")\n",
    "        axes2[row, col].set_ylabel(\"Weight\")\n",
    "        axes2[row, col].set_ylim(min_w, max_w)\n",
    "    plt.suptitle(f\"Hidden Two Weights - Epoch {epoch+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path_hidden_two)\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 3: Classification Weights (2x5 grid of line plots)\n",
    "    fig3, axes3 = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    min_w = classification_w.min()\n",
    "    max_w = classification_w.max()\n",
    "    for j in range(10):\n",
    "        row, col = divmod(j, 5)\n",
    "        axes3[row, col].plot(classification_w[j])\n",
    "        axes3[row, col].set_title(f\"Class {j}\")\n",
    "        axes3[row, col].set_xlabel(\"From H2 Neuron\")\n",
    "        axes3[row, col].set_ylabel(\"Weight\")\n",
    "        axes3[row, col].set_ylim(min_w, max_w)\n",
    "    plt.suptitle(f\"Classification Weights - Epoch {epoch+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path_classification)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Once again, two hidden layers. 16 nodes each. Same as 3blue1brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_size_by_pixels = 28*28\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # define layers separately to have access to each\n",
    "        self.hidden_one = nn.Linear(layer_size_by_pixels, 16)\n",
    "        self.hidden_two = nn.Linear(16, 16)\n",
    "        self.classification_layer = nn.Linear(16, 10)\n",
    "        \n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # first hidden layer\n",
    "        hidden_one_out = self.hidden_one(x)\n",
    "        hidden_one_act = self.activation_function(hidden_one_out)\n",
    "\n",
    "        # second hidden layer\n",
    "        hidden_two_out = self.hidden_two(hidden_one_act)\n",
    "        hidden_two_act = self.activation_function(hidden_two_out)\n",
    "\n",
    "        # classification layer\n",
    "        classification_out = self.classification_layer(hidden_two_act)\n",
    "        \n",
    "        return classification_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights (to compare below): [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model weights (to compare below): {model.hidden_one.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First set of weights: [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n",
      "Second set of weights: [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n",
      "Are the two sets equal: [ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.hidden_one.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.hidden_one.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx].copy()).float(),\n",
    "            torch.tensor(self.labels[idx].copy(), dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 1.0223\n",
      "  Val Loss: 0.5656 | Val Acc: 83.62%\n",
      "  Saved model with Val Acc: 83.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "  Train Loss: 0.4646\n",
      "  Val Loss: 0.4460 | Val Acc: 86.46%\n",
      "  Saved model with Val Acc: 86.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "  Train Loss: 0.3924\n",
      "  Val Loss: 0.3987 | Val Acc: 88.16%\n",
      "  Saved model with Val Acc: 88.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "  Train Loss: 0.3673\n",
      "  Val Loss: 0.3996 | Val Acc: 88.18%\n",
      "  Saved model with Val Acc: 88.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "  Train Loss: 0.3517\n",
      "  Val Loss: 0.3552 | Val Acc: 89.42%\n",
      "  Saved model with Val Acc: 89.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "  Train Loss: 0.3349\n",
      "  Val Loss: 0.3484 | Val Acc: 89.91%\n",
      "  Saved model with Val Acc: 89.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "  Train Loss: 0.3278\n",
      "  Val Loss: 0.3521 | Val Acc: 89.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "  Train Loss: 0.3185\n",
      "  Val Loss: 0.3430 | Val Acc: 89.99%\n",
      "  Saved model with Val Acc: 89.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "  Train Loss: 0.3092\n",
      "  Val Loss: 0.3521 | Val Acc: 89.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "  Train Loss: 0.3059\n",
      "  Val Loss: 0.3405 | Val Acc: 90.10%\n",
      "  Saved model with Val Acc: 90.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 0.2975\n",
      "  Val Loss: 0.3421 | Val Acc: 90.39%\n",
      "  Saved model with Val Acc: 90.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "  Train Loss: 0.2959\n",
      "  Val Loss: 0.3340 | Val Acc: 90.59%\n",
      "  Saved model with Val Acc: 90.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "  Train Loss: 0.2922\n",
      "  Val Loss: 0.3310 | Val Acc: 90.67%\n",
      "  Saved model with Val Acc: 90.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "  Train Loss: 0.2875\n",
      "  Val Loss: 0.3456 | Val Acc: 89.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "  Train Loss: 0.2879\n",
      "  Val Loss: 0.3609 | Val Acc: 89.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "  Train Loss: 0.2835\n",
      "  Val Loss: 0.3243 | Val Acc: 90.88%\n",
      "  Saved model with Val Acc: 90.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "  Train Loss: 0.2821\n",
      "  Val Loss: 0.3349 | Val Acc: 90.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "  Train Loss: 0.2784\n",
      "  Val Loss: 0.3256 | Val Acc: 90.95%\n",
      "  Saved model with Val Acc: 90.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "  Train Loss: 0.2754\n",
      "  Val Loss: 0.3458 | Val Acc: 89.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "  Train Loss: 0.2749\n",
      "  Val Loss: 0.3339 | Val Acc: 90.36%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # set the model to training mode - this is currently a no-op\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "    for batch in train_bar:\n",
    "        # deconstruct batch items\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out = model(images)\n",
    "        \n",
    "        # --- Loss and Backprop ---\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = classification_loss_fn(classification_out, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress\n",
    "        train_loss += total_loss.item()\n",
    "        train_bar.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            # deconstruct\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            classification_out = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # epoch stats\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_val_loss = avg_val_loss  # Update loss for reference\n",
    "        torch.save(model.state_dict(), f'best_model_baseline_{epoch}.pth')\n",
    "        print(f\"  Saved model with Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "    # Optional: Save if accuracy is equal but loss is lower\n",
    "    elif val_accuracy == best_val_acc and avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f'best_model_baseline_{epoch}.pth')\n",
    "        print(f\"  Saved model with same Val Acc: {val_accuracy:.2f}% but lower Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    plot_weights(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3ea2-fd60-4e54-8942-33172e4de452",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43901c32-73a7-4236-876a-fd802d9a0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "    for batch in test_bar:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        classification_out = model(images)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c324f81-daec-4475-9033-10ef337e06e7",
   "metadata": {},
   "source": [
    "# Exploring the Resulting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa939e-845d-4c0a-a0d4-2c2fb6c64ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
