{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# Alt Architecture - More Subfeatures! Now we introduce them! Lower loss! Sharp subfeatures! Only applied to first layer!\n",
    "\n",
    "How big of a difference does it make using exact subfeatures instead of average subfeatures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, load_intermediate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"./data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"./data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"./data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"./data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (50000, 28, 28)\n",
      "Val images shape: (10000, 28, 28)\n",
      "Test images shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd06cc-fd16-4933-be26-030360e3a363",
   "metadata": {},
   "source": [
    "## Loading Subfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1158523f-edd9-4a33-a5f3-150d98d7f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features = load_intermediate_labels(\"min_sub_feature_dict_v2.pkl\")\n",
    "sub_features = load_intermediate_labels(\"sub_feature_dict_v2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f56082e-cf3b-4306-a6da-e4495d31ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features = [torch.tensor(sf, dtype=torch.float32) for sf in min_sub_features]\n",
    "sub_features = [torch.tensor(sf, dtype=torch.float32) for sf in sub_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18aa3002-845f-4dbf-b5b4-04ce5788caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sub_features_tensor = torch.stack(min_sub_features).to(device)\n",
    "sub_features_tensor = torch.stack(sub_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b48d03-7d6a-4e95-b1ae-2a07f55eed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200000, 28, 28]), torch.Size([100000, 28, 28]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sub_features_tensor.shape, sub_features_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517abc1-b197-47cc-b71a-921b059ba56d",
   "metadata": {},
   "source": [
    "# Our Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We keep our two hidden layers at image size to be able to calculate a local loss to push those layers to learn human recognizable structures. However, for the example below, we don't calculate intermediate loss, as we need a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        layer_size_by_pixels = 28 * 28\n",
    "        \n",
    "        self.first_layer = nn.ModuleList([nn.Linear(layer_size_by_pixels, layer_size_by_pixels) for _ in range(4)])\n",
    "        self.second_layer = nn.ModuleList([nn.Linear(layer_size_by_pixels, layer_size_by_pixels) for _ in range(2)])\n",
    "        self.classification_layer = nn.Linear(layer_size_by_pixels, 10)\n",
    "        self.activation_function = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        first_layer_outputs = []\n",
    "        for layer in self.first_layer:\n",
    "            out = layer(x)\n",
    "            act = self.activation_function(out)\n",
    "            first_layer_outputs.append(act)\n",
    "        \n",
    "        second_layer_outputs = []\n",
    "        for i in range(2):\n",
    "            combined_input = first_layer_outputs[2*i] + first_layer_outputs[2*i + 1]\n",
    "            out = self.second_layer[i](combined_input)\n",
    "            act = self.activation_function(out)\n",
    "            second_layer_outputs.append(act)\n",
    "        \n",
    "        # Combine second_layer_outputs for classification\n",
    "        combined_act = sum(second_layer_outputs)\n",
    "        \n",
    "        classification_out = self.classification_layer(combined_act)\n",
    "        \n",
    "        # Return classification_out and all activations\n",
    "        return classification_out, first_layer_outputs, second_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "sub_feature_loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights (to compare below): [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model weights (to compare below): {model.first_layer[0].weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First set of weights: [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n",
      "Second set of weights: [ 0.02730495  0.02964314 -0.00836687  0.03280755 -0.00782513]\n",
      "Are the two sets equal: [ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.first_layer[0].weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.first_layer[0].weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx].copy()).float(),\n",
    "            torch.tensor(self.labels[idx].copy(), dtype=torch.long),\n",
    "            idx\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 7.0791\n",
      "  Val Loss: 0.2054 | Val Acc: 94.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "  Train Loss: 3.6338\n",
      "  Val Loss: 0.1762 | Val Acc: 95.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "  Train Loss: 3.0591\n",
      "  Val Loss: 0.1660 | Val Acc: 95.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "  Train Loss: 2.6843\n",
      "  Val Loss: 0.1676 | Val Acc: 96.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "  Train Loss: 2.4680\n",
      "  Val Loss: 0.1987 | Val Acc: 94.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "  Train Loss: 2.3432\n",
      "  Val Loss: 0.2127 | Val Acc: 95.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "  Train Loss: 2.2552\n",
      "  Val Loss: 0.2473 | Val Acc: 96.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "  Train Loss: 2.1390\n",
      "  Val Loss: 0.2383 | Val Acc: 96.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "  Train Loss: 2.0686\n",
      "  Val Loss: 0.2852 | Val Acc: 95.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "  Train Loss: 1.9707\n",
      "  Val Loss: 0.3165 | Val Acc: 95.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 2.0383\n",
      "  Val Loss: 0.2724 | Val Acc: 96.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "  Train Loss: 1.9791\n",
      "  Val Loss: 0.2414 | Val Acc: 96.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "  Train Loss: 2.0486\n",
      "  Val Loss: 0.2669 | Val Acc: 96.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "  Train Loss: 1.9853\n",
      "  Val Loss: 0.2981 | Val Acc: 96.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "  Train Loss: 2.0046\n",
      "  Val Loss: 0.3270 | Val Acc: 95.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "  Train Loss: 2.0352\n",
      "  Val Loss: 0.3021 | Val Acc: 96.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "  Train Loss: 1.9857\n",
      "  Val Loss: 0.3124 | Val Acc: 96.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "  Train Loss: 2.0348\n",
      "  Val Loss: 0.3728 | Val Acc: 96.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "  Train Loss: 2.0266\n",
      "  Val Loss: 0.4197 | Val Acc: 95.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "  Train Loss: 2.0295\n",
      "  Val Loss: 0.4688 | Val Acc: 96.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "loss_factor = 0.001\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # set the model to training mode - this is currently a no-op\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "    for batch in train_bar:\n",
    "        # deconstruct batch items\n",
    "        images, labels, indices = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # Compute subfeature indices (keep on CPU for tensor indexing)\n",
    "        first_layer_indices = torch.cat([4 * idx + torch.arange(4) for idx in indices])\n",
    "        second_layer_indices = torch.cat([2 * idx + torch.arange(2) for idx in indices])\n",
    "    \n",
    "        # Extract subfeatures and reshape\n",
    "        first_layer_labels = min_sub_features_tensor[first_layer_indices].view(batch_size, 4, 784)\n",
    "        second_layer_labels = sub_features_tensor[second_layer_indices].view(batch_size, 2, 784)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, first_layer_outputs, second_layer_outputs = model(images)\n",
    "        \n",
    "        # --- Loss and Backprop ---\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # subfeature loss\n",
    "        first_loss = sum(sub_feature_loss_fn(out, first_layer_labels[:, i, :]) for i, out in enumerate(first_layer_outputs))\n",
    "        second_loss = sum(sub_feature_loss_fn(out, second_layer_labels[:, i, :]) for i, out in enumerate(second_layer_outputs))\n",
    "\n",
    "        # classification loss\n",
    "        classification_loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "        # total loss\n",
    "        total_loss = loss_factor * (first_loss) + classification_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress\n",
    "        train_loss += total_loss.item()\n",
    "        train_bar.set_postfix(loss=classification_loss.item())\n",
    "\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            # deconstruct\n",
    "            images, labels, _  = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            classification_out, _, _ = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # epoch stats\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3ea2-fd60-4e54-8942-33172e4de452",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43901c32-73a7-4236-876a-fd802d9a0f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 246.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 95.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "    for batch in test_bar:\n",
    "        images, labels, _ = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(images)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c324f81-daec-4475-9033-10ef337e06e7",
   "metadata": {},
   "source": [
    "# Exploring the Resulting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec20844-c10d-44ba-bb98-2d5f3c1fdfe4",
   "metadata": {},
   "source": [
    "## Visualizing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f68f04-df2e-4577-9e42-2bb8b97e6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_w = np.abs(model.classification_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "vertical_w = np.abs(model.vertical_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "horizontal_w = np.abs(model.horizontal_layer.weight[0].reshape(28, 28).detach().cpu().numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 5))\n",
    "\n",
    "visualize_image(horizontal_w, \"Horizontal Layer Weights\", ax=axes[0])\n",
    "visualize_image(vertical_w, \"Vertical Layer Weights\", ax=axes[1])\n",
    "visualize_image(classification_w, \"Classification Layer Weights\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a190a-d3bb-4d84-8124-ea9f6d5f6e10",
   "metadata": {},
   "source": [
    "# Visualizing Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98175107-033e-43be-a43b-6a30ec12f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_examples = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_images[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy()).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, first_act_layer, second_act_layer = model(img_tensor)\n",
    "        \n",
    "        first_act_imgs  = [act.clone().reshape(28, 28).detach().cpu().numpy() for act in first_act_layer]\n",
    "        second_act_imgs = [act.clone().reshape(28, 28).detach().cpu().numpy() for act in second_act_layer]\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        eval_examples.append((label, img, out, first_act_imgs, second_act_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c8f1-1b0b-43da-8672-2a6e99c8fb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, first_act_imgs, second_act_imgs in eval_examples:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\")\n",
    "    for img in first_act_imgs:\n",
    "        visualize_image(img, \"First Layer Activations\")\n",
    "\n",
    "    for img in second_act_imgs:\n",
    "        visualize_image(img, \"Second Layer Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088160a3-3fd3-4fcf-8c74-f0ca4d6bb1a8",
   "metadata": {},
   "source": [
    "# Different Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25df7e0-a134-4f55-9b09-556d00635870",
   "metadata": {},
   "source": [
    "## Horizontal Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79301b-04ae-46bf-8227-da54576676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "h_edge_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_horizontal_image_labels[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        h_edge_inputs.append((label, img.copy().reshape(28, 28), out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2db2f-72ad-4a5c-9301-7d24ce5bf4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in h_edge_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5816bd-a1ca-48d0-9b7d-6a014501de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        if len(horizontal_labels) == 128:\n",
    "            h_imgs = horizontal_labels.reshape(128, 28, 28)\n",
    "        else:\n",
    "            h_imgs = horizontal_labels.reshape(16, 28, 28)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(h_imgs)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d289469-a61e-40ba-8892-3422f3f0c063",
   "metadata": {},
   "source": [
    "## Vertical Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62c544-1f0f-4de2-ba71-a7a8d9304063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "v_edge_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(test_vertical_image_labels[:10]):\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = test_labels[idx]\n",
    "        v_edge_inputs.append((label, img.copy().reshape(28, 28), out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace73b9-4edd-41c2-853c-0be864db4a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in v_edge_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718757a8-451c-4fea-b923-2b5628b122ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels, horizontal_labels, vertical_labels = batch\n",
    "        images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "        if len(horizontal_labels) == 128:\n",
    "            v_imgs = vertical_labels.reshape(128, 28, 28)\n",
    "        else:\n",
    "            v_imgs = vertical_labels.reshape(16, 28, 28)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(v_imgs)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfbcfd-16fa-4209-90b9-537a0439515f",
   "metadata": {},
   "source": [
    "There might be something to vertical edges being more important features than horizontal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeeab6-27af-431c-ba51-de847761a24d",
   "metadata": {},
   "source": [
    "## Random Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916d629-0ea5-4e52-90f7-80d73d0128f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 28, 28\n",
    "noisy_images = [np.random.randint(0, 256, (height, width), dtype=np.uint8) for _ in range(0, 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291e3df-effe-4d1b-ac9b-dc050f0634c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noisy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22e937-3a73-4707-b698-653181e550d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(noisy_images[0], \"Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bdad8-1a03-4110-9026-0601790cf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "noisy_inputs = list()\n",
    "with torch.no_grad():\n",
    "    for img in noisy_images[:10]:\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "        out, v_act, h_act = model(img_tensor)\n",
    "        \n",
    "        v_act_img = v_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "        h_act_img = h_act.clone().reshape(28, 28).detach().cpu().numpy()\n",
    "\n",
    "        label = \"Noise\"\n",
    "        noisy_inputs.append((label, img, out, v_act_img, h_act_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288fc23-5e83-457c-996b-7d481fc5c70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = list(range(0, 10))\n",
    "for label, img, out, v_act_img, h_act_img in noisy_inputs:\n",
    "    inference = torch.max(out, 1)[1].item()\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "\n",
    "    visualize_image(img, f\"Input Img - {label}\", ax=axes[0])\n",
    "    visualize_image(h_act_img, \"Horizontal Activations\", ax=axes[1])\n",
    "    visualize_image(v_act_img, \"Vertical Activations\", ax=axes[2])\n",
    "\n",
    "    ax3 = plt.subplot(1, 4, 4)\n",
    "    bars3 = ax3.bar(x_values, out.tolist()[0])\n",
    "    ax3.set_xticks(x_values)\n",
    "    ax3.set_title(f\"Activation max: {inference}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712b455-e881-4784-b71c-b8957e59c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # again currently a no-op\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "answer_dict = {}\n",
    "activation_dict = {}\n",
    "with torch.no_grad():\n",
    "    for img in noisy_images:\n",
    "        img_tensor = torch.from_numpy(img.copy().reshape(28, 28)).float().unsqueeze(0).unsqueeze(0).to('cuda')\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, _, _ = model(img_tensor)\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "\n",
    "        for i, act in enumerate(classification_out[0]):\n",
    "            if i in activation_dict:\n",
    "                activation_dict[i] += act.item()\n",
    "            else:\n",
    "                activation_dict[i] = act.item()\n",
    "\n",
    "        if predicted.item() in answer_dict:\n",
    "            answer_dict[predicted.item()] += 1\n",
    "        else:\n",
    "            answer_dict[predicted.item()] = 1\n",
    "        \n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f129990-5e26-4c0b-9e60-7fe43f1bf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d27a5e-6908-4dce-b917-4f754ffc1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(key, round(value, 2)) for key, value in activation_dict.items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b5408-b96b-421a-9bf3-be86c9f088de",
   "metadata": {},
   "source": [
    "Honestly, not a terrible distribution, but it seems our low loss factor is \"best\" in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f4b16-d7af-41ad-8e7f-8e72ee1afedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e553352-62e8-4106-86e0-b277147b7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [str(x) for x in range(0, 10)]\n",
    "x = np.array([int(k) for k in activation_dict.keys()])  # positions for the categories\n",
    "y = np.array([int(v) for v in activation_dict.values()])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the first set of bars\n",
    "bars1 = ax.bar(x, y)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Digit Classes')\n",
    "ax.set_ylabel('Summed Activations')\n",
    "ax.set_title('Summed Activations vs. Digit Class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466336ff-07eb-445c-9902-87dd38ae019e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
