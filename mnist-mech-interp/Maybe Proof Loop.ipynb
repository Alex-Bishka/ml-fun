{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# MNIST (Partial) Side Quests?\n",
    "\n",
    "This notebook builds off the ideas from the `MNIST Side Quests` notebook. In this experiment, we allow the network to have some more freedom in how to use the \"side quest\" information. Instead of using the sub losses to take a step for each sub layer, we will use the sub losses to add to the total loss. This is somewhere between pursuing just the main quest and pursuing the side quests, at least 50% effort (for each side quest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, generate_intermediate_edge_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8621412-5edb-467f-82a5-91f188b8198d",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f2e5c-66a1-43b2-b847-843648d06b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"./loop-model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"./data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"./data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"./data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"./data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca05d5-d791-4fbb-9736-b52a5eae1409",
   "metadata": {},
   "source": [
    "## Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f14c-5f30-4237-933c-55a6f254a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = train_images[0]\n",
    "sample_label = train_labels[0]\n",
    "visualize_image(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623254-d358-4eeb-a535-dda1b17ffcc6",
   "metadata": {},
   "source": [
    "## Convolutions to get Horizontal Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e926333-e915-4f3b-b0bb-60c61233e346",
   "metadata": {},
   "source": [
    "### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe86a5-6cc9-4b4b-89b4-c65e27f34aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_kernel = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [ 0,  0 , 0],\n",
    "    [ 1,  1,  1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113ba4a-1a57-4bdf-a32b-25e2018e1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_edges = get_edges(horizontal_kernel, sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5120d1-8b77-43f8-924b-0de360a8ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(horizontal_edges, f\"{sample_label} horizontal edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e610f8-ea28-41cd-8d8a-6fb8878e29b1",
   "metadata": {},
   "source": [
    "### Extending the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe6896-5ce3-45de-b4f4-4a837a5bb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_horizontal_image_labels = generate_intermediate_edge_labels(train_images, horizontal_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f888ca-0507-4256-9deb-5377e7147f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_horizontal_image_labels = generate_intermediate_edge_labels(val_images, horizontal_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87afaea-31e9-4e75-85e2-bd963aaf59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_horizontal_image_labels = generate_intermediate_edge_labels(test_images, horizontal_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b07c68-0b2f-40ed-92bd-5c0752f43da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(train_horizontal_image_labels[0].reshape(28, 28), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1e874-dbd8-4306-891b-e4b543723dbc",
   "metadata": {},
   "source": [
    "## Convolutions to get Vertical Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66748039-c750-43b5-8388-7c91f61224ea",
   "metadata": {},
   "source": [
    "### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67c8b7-c172-499e-abe5-05033a5c7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_kernel = np.array([\n",
    "    [-1,  0,  1],\n",
    "    [-1,  0,  1],\n",
    "    [-1,  0,  1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7385e-91ba-4223-bbed-83b309a4578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edges = get_edges(vertical_kernel, sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30570246-eed3-4f7a-8251-d06c418af8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(vertical_edges, f\"{sample_label} vertical edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820c65c-02ac-4bbb-b9cc-d22fc2f2673d",
   "metadata": {},
   "source": [
    "### Extending the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d0eaa-60cb-4e2b-a26c-21fada7d712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vertical_image_labels = generate_intermediate_edge_labels(train_images, vertical_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d3563-aae4-464d-aded-bae41638e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vertical_image_labels = generate_intermediate_edge_labels(val_images, vertical_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c68ad-3ee2-4755-ab60-324d554fdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vertical_image_labels = generate_intermediate_edge_labels(test_images, vertical_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c116c99-a6bf-4c89-8290-e71eb673e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(train_vertical_image_labels[0].reshape(28, 28), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517abc1-b197-47cc-b71a-921b059ba56d",
   "metadata": {},
   "source": [
    "# Our Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We keep our two hidden layers at image size to be able to calculate a local loss to push those layers to learn human recognizable structures. However, for the example below, we don't calculate intermediate loss, as we need a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_size_by_pixels = 28*28\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # define layers separately to have access to each\n",
    "        self.horizontal_layer = nn.Linear(layer_size_by_pixels, layer_size_by_pixels)\n",
    "        self.vertical_layer = nn.Linear(layer_size_by_pixels, layer_size_by_pixels)\n",
    "        self.classification_layer = nn.Linear(layer_size_by_pixels, 10)\n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # horizontal layer\n",
    "        horizontal_out = self.horizontal_layer(x)\n",
    "        horizontal_act = self.activation_function(horizontal_out)\n",
    "\n",
    "        # vertical layer connected to horizontal layer\n",
    "        vertical_out = self.vertical_layer(horizontal_act)\n",
    "        vertical_act = self.activation_function(vertical_out)\n",
    "\n",
    "        # detached verision for vertical loss only\n",
    "        # detached_input = horizontal_act.detach()\n",
    "        # vertical_out_detached = self.vertical_layer(detached_input)\n",
    "        # vertical_act_detached = self.activation_function(vertical_out_detached)\n",
    "\n",
    "        # coupling layer\n",
    "        classification_out = self.classification_layer(vertical_act)\n",
    "        \n",
    "        return classification_out, vertical_act, horizontal_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "horizontal_loss_fn = nn.MSELoss()\n",
    "vertical_loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model weights (to compare below): {model.horizontal_layer.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.horizontal_layer.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.horizontal_layer.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels, horizontal_edges, vertical_edges):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.horizontal_edges = horizontal_edges\n",
    "        self.vertical_edges = vertical_edges\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx]).float(),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            torch.from_numpy(self.horizontal_edges[idx]).float(),\n",
    "            torch.from_numpy(self.vertical_edges[idx]).float(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels, train_horizontal_image_labels, train_vertical_image_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels, val_horizontal_image_labels, val_vertical_image_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels, test_horizontal_image_labels, test_vertical_image_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "steps = [x / 1000 for x in list(range(0, 200 + 1, 1))]\n",
    "runs = dict()\n",
    "for step in steps:\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()  # set the model to training mode - this is currently a no-op\n",
    "        train_loss = 0.0\n",
    "    \n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "        for batch in train_bar:\n",
    "            # deconstruct batch items\n",
    "            images, labels, horizontal_labels, vertical_labels = batch\n",
    "            images, labels, horizontal_labels, vertical_labels = images.to(device), labels.to(device), horizontal_labels.to(device), vertical_labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            classification_out, vertical_act, horizontal_act = model(images)\n",
    "            \n",
    "            # --- Loss and Backprop ---\n",
    "    \n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # vertical loss\n",
    "            vertical_loss = vertical_loss_fn(vertical_act, vertical_labels)\n",
    "    \n",
    "            # horizontal loss\n",
    "            horizontal_loss = horizontal_loss_fn(horizontal_act, horizontal_labels)\n",
    "    \n",
    "            # classification loss\n",
    "            classification_loss = classification_loss_fn(classification_out, labels)\n",
    "    \n",
    "            # total loss\n",
    "            total_loss = step * (vertical_loss + horizontal_loss) + classification_loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "    \n",
    "            # update progress\n",
    "            train_loss += total_loss.item()\n",
    "            train_bar.set_postfix(loss=classification_loss.item())\n",
    "    \n",
    "        \n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_bar:\n",
    "                # deconstruct\n",
    "                images, labels, _, _ = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                # forward pass\n",
    "                classification_out, _, _ = model(images)\n",
    "    \n",
    "                # compute loss\n",
    "                loss = classification_loss_fn(classification_out, labels)\n",
    "    \n",
    "                # calculate metrics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(classification_out, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        # epoch stats\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_horizontal_loss = horizontal_loss / len(train_loader)\n",
    "        avg_vertical_loss = vertical_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Horizontal Edge Loss: {avg_horizontal_loss:.4f}\")\n",
    "        print(f\"  Vertical Edge Loss: {avg_vertical_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "        # save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_NAME)\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_NAME))\n",
    "    model.eval()  # again currently a no-op\n",
    "    \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(val_loader, desc=f\"Evaluation\")\n",
    "        for batch in test_bar:\n",
    "            images, labels, _, _ = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "            # forward pass\n",
    "            classification_out, _, _ = model(images)\n",
    "    \n",
    "            # stats\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    runs[step] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e0651-3ab0-40ca-93a2-47de9c94ae69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d035ed9-77cf-410a-aba0-169eca079b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
