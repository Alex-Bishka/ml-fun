{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# Training on SAE Sparse Features\n",
    "\n",
    "Loss factor of 0.01, working with the 25 highest activating features per digit\n",
    "\n",
    "Cosine instead of MSE for auxilary loss - it seemed to work quite well, and makes auxiliary loss easier to manage w/global loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa37d0-59c1-4368-aec0-83ccb0ce6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "EXPERIMENT_TYPE = \"SAE\"\n",
    "RUN_ID = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b67228-330c-480e-99b5-cb4001e89013",
   "metadata": {},
   "source": [
    "## SAE Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d7778-885a-4a6d-9bd2-ddbcb9a27be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "L1_PENALTY = 0.01\n",
    "N = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7038c3-06d1-4d3c-9b44-2f2bc3485f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_helpers import (plot_weights,\n",
    "                    plot_activations,\n",
    "                    plot_losses,\n",
    "                    plot_saliency_map,\n",
    "                    plot_sparse_vecs_by_image,\n",
    "                    plot_top_act_images_by_feature,\n",
    "                    feature_inversion\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c069cc3-458e-4825-9828-c771c7f7370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# assume cwd is project_root/data_loader\n",
    "project_root = Path(os.getcwd()).parent  # go up one level to project_root\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, load_intermediate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_intermediate_labels(\"first_layer/train_images.pkl\")\n",
    "train_labels = load_intermediate_labels(\"first_layer/train_labels.pkl\")\n",
    "\n",
    "# val data\n",
    "val_images = load_intermediate_labels(\"first_layer/val_images.pkl\")\n",
    "val_labels = load_intermediate_labels(\"first_layer/val_labels.pkl\")\n",
    "\n",
    "# test data\n",
    "test_images = load_intermediate_labels(\"first_layer/test_images.pkl\")\n",
    "test_labels = load_intermediate_labels(\"first_layer/test_labels.pkl\")\n",
    "\n",
    "# intermediate labels\n",
    "recon_max_sparse_act_one = load_intermediate_labels(f\"first_layer/train_images_recon_max_sparse_top_{N}.pkl\")\n",
    "# recon_max_sparse_act_one_ablated = load_intermediate_labels(f\"first_layer/train_images_recon_max_sparse_ablated_top_{N}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de73dd-39e5-4c74-b3a3-03b6155d3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(recon_max_sparse_act_one[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca05d5-d791-4fbb-9736-b52a5eae1409",
   "metadata": {},
   "source": [
    "## Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f14c-5f30-4237-933c-55a6f254a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = train_images[0]\n",
    "sample_label = train_labels[0]\n",
    "visualize_image(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "### NN\n",
    "\n",
    "Once again, two hidden layers. 16 nodes each. Same as 3blue1brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_size_by_pixels = 28*28\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # define layers separately to have access to each\n",
    "        self.hidden_one = nn.Linear(layer_size_by_pixels, 16)\n",
    "        self.hidden_two = nn.Linear(16, 16)\n",
    "        self.classification_layer = nn.Linear(16, 10)\n",
    "        \n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # first hidden layer\n",
    "        hidden_one_out = self.hidden_one(x)\n",
    "        hidden_one_act = self.activation_function(hidden_one_out)\n",
    "\n",
    "        # second hidden layer\n",
    "        hidden_two_out = self.hidden_two(hidden_one_act)\n",
    "        hidden_two_act = self.activation_function(hidden_two_out)\n",
    "\n",
    "        # classification layer\n",
    "        classification_out = self.classification_layer(hidden_two_act)\n",
    "        \n",
    "        return classification_out, hidden_one_act, hidden_two_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44c3cf-602d-451e-b10c-8b7f005858e8",
   "metadata": {},
   "source": [
    "### SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53786553-7c2c-4d0f-96a9-b901297c7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=16, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.activation(self.encoder(x))\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed, encoded\n",
    "    \n",
    "    def loss(self, x, reconstructed, encoded, l1_lambda=0.001):\n",
    "        mse_loss = nn.MSELoss()(reconstructed, x)\n",
    "        l1_loss = l1_lambda * torch.mean(torch.abs(encoded))\n",
    "        return mse_loss + l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels, first_layer_acts=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.first_layer_acts = first_layer_acts\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        if self.first_layer_acts:\n",
    "            assert(len(self.first_layer_acts) == len(self.images))\n",
    "            \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(self.images[idx].copy()).float()\n",
    "        label = torch.tensor(self.labels[idx].copy(), dtype=torch.long)\n",
    "\n",
    "        if self.first_layer_acts:\n",
    "            return (image, label, self.first_layer_acts[idx])\n",
    "        else:\n",
    "            return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0673ee-7f67-474d-a0f1-a364abdde520",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 0.01\n",
    "max_loss = 0.10\n",
    "step = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a2e09-4d54-4b9a-8abe-379edef96d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_factors = np.arange(min_loss, max_loss + step, step)\n",
    "print(len(loss_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efead09-bdd4-4177-9ead-fee16bb28a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_data_dict = {}\n",
    "for loss_factor in loss_factors:\n",
    "    print(\"#\" * 50)\n",
    "    print(f\"Loss factor: {loss_factor}\\n\\n\")\n",
    "    ######################################################################################################\n",
    "    # MODELS INIT\n",
    "    ######################################################################################################\n",
    "    \n",
    "    # for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    model = NeuralNetwork().to(device)\n",
    "    \n",
    "    # loss functions\n",
    "    classification_loss_fn = nn.CrossEntropyLoss()\n",
    "    hidden_act_one_loss_fn = nn.CosineSimilarity()\n",
    "    \n",
    "    # optimizers\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    sae_hidden_two = SparseAutoencoder(input_size=16, hidden_size=HIDDEN_SIZE).to(device)\n",
    "    optimizer_sae_hidden_two = torch.optim.Adam(sae_hidden_two.parameters())\n",
    "    \n",
    "    # for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    sae_hidden_one = SparseAutoencoder(input_size=16, hidden_size=HIDDEN_SIZE).to(device)\n",
    "    optimizer_sae_hidden_one = torch.optim.Adam(sae_hidden_one.parameters())\n",
    "    \n",
    "    ######################################################################################################\n",
    "    # DATA INIT\n",
    "    ######################################################################################################\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    NUM_WORKERS = 4\n",
    "    if device.type.lower() == \"cpu\":\n",
    "        NUM_WORKERS = 0\n",
    "    \n",
    "    # training data\n",
    "    train_dataset = EdgeDataset(train_images, train_labels, recon_max_sparse_act_one_ablated)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=generator)\n",
    "    \n",
    "    # validation data\n",
    "    val_dataset = EdgeDataset(val_images, val_labels)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)  # larger batch size for faster validation\n",
    "    \n",
    "    # test data\n",
    "    test_dataset = EdgeDataset(test_images, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    # TRAINING LOOP\n",
    "    ######################################################################################################\n",
    "    best_model = None\n",
    "    best_sae_one = None\n",
    "    best_sae_two = None\n",
    "    num_epochs = 20\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "    SAE_hidden_one_losses = []\n",
    "    SAE_hidden_two_losses = []\n",
    "    \n",
    "    # Initialize storage for training features and labels\n",
    "    feature_activations_one_train = torch.zeros(HIDDEN_SIZE, len(train_loader.dataset))\n",
    "    feature_activations_two_train = torch.zeros(HIDDEN_SIZE, len(train_loader.dataset))\n",
    "    labels_train = torch.zeros(len(train_loader.dataset), dtype=torch.long)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()  # set the model to training mode - this is currently a no-op\n",
    "        sae_hidden_two.train()\n",
    "        sae_hidden_one.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        total_sae_loss_hidden_two = 0.0\n",
    "        total_sae_loss_hidden_one = 0.0\n",
    "    \n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "        for batch_idx, batch in enumerate(train_bar):\n",
    "            # deconstruct batch items\n",
    "            images, labels, acts_one = batch\n",
    "            images, labels, acts_one = images.to(device), labels.to(device), acts_one.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            classification_out, hidden_act_one, hidden_act_two = model(images)\n",
    "    \n",
    "            # Classification loss and backprop\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            sub_loss = (1 - hidden_act_one_loss_fn(hidden_act_one, acts_one)).mean()\n",
    "            total_loss = classification_loss_fn(classification_out, labels) + loss_factor * (sub_loss)\n",
    "            total_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item()\n",
    "            train_bar.set_postfix(loss=total_loss.item())\n",
    "    \n",
    "            # to prevent backprop on both graphs:\n",
    "            hidden_act_one_detached = hidden_act_one.detach()\n",
    "            hidden_act_two_detached = hidden_act_two.detach()\n",
    "    \n",
    "            # SAE loss and backprop - hidden layer one\n",
    "            optimizer_sae_hidden_one.zero_grad()\n",
    "            reconstructed_one, encoded_one = sae_hidden_one(hidden_act_one_detached)\n",
    "            sae_loss_hidden_one = sae_hidden_one.loss(hidden_act_one_detached,\n",
    "                                                      reconstructed_one,\n",
    "                                                      encoded_one,\n",
    "                                                      l1_lambda=L1_PENALTY\n",
    "                                                     )\n",
    "            sae_loss_hidden_one.backward()\n",
    "            optimizer_sae_hidden_one.step()\n",
    "            total_sae_loss_hidden_one += sae_loss_hidden_one.item()\n",
    "            \n",
    "            # SAE loss and backprop - hidden layer two\n",
    "            optimizer_sae_hidden_two.zero_grad()\n",
    "            reconstructed_two, encoded_two = sae_hidden_two(hidden_act_two_detached)\n",
    "            sae_loss_hidden_two = sae_hidden_two.loss(hidden_act_two_detached,\n",
    "                                                      reconstructed_two,\n",
    "                                                      encoded_two,\n",
    "                                                      l1_lambda=L1_PENALTY\n",
    "                                                     )\n",
    "            sae_loss_hidden_two.backward()\n",
    "            optimizer_sae_hidden_two.step()\n",
    "            total_sae_loss_hidden_two += sae_loss_hidden_two.item()\n",
    "    \n",
    "            # Store training features and labels\n",
    "            start_idx = batch_idx * train_loader.batch_size\n",
    "            end_idx = start_idx + images.size(0)\n",
    "            feature_activations_one_train[:, start_idx:end_idx] = encoded_one.T.cpu()\n",
    "            feature_activations_two_train[:, start_idx:end_idx] = encoded_two.T.cpu()\n",
    "            labels_train[start_idx:end_idx] = labels.cpu()\n",
    "    \n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_bar:\n",
    "                # deconstruct\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                # forward pass\n",
    "                classification_out, _, _ = model(images)\n",
    "    \n",
    "                # compute loss\n",
    "                loss = classification_loss_fn(classification_out, labels)\n",
    "    \n",
    "                # calculate metrics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(classification_out, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        # epoch stats\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_sae_train_loss_hidden_one = total_sae_loss_hidden_one / len(train_loader)\n",
    "        avg_sae_train_loss_hidden_two = total_sae_loss_hidden_two / len(train_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  SAE Train Loss (hidden one): {avg_sae_train_loss_hidden_one:.4f}\")\n",
    "        print(f\"  SAE Train Loss (hidden two): {avg_sae_train_loss_hidden_two:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "        model_path = f'./models/{EXPERIMENT_TYPE}/{RUN_ID}/best_model_baseline_{epoch+1}.pth'\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_val_loss = avg_val_loss  # Update loss for reference\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  Saved model with Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_sae_one = copy.deepcopy(sae_hidden_one)\n",
    "            best_sae_two = copy.deepcopy(sae_hidden_two)\n",
    "            \n",
    "        # Optional: Save if accuracy is equal but loss is lower\n",
    "        elif val_accuracy == best_val_acc and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  Saved model with same Val Acc: {val_accuracy:.2f}% but lower Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_sae_one = copy.deepcopy(sae_hidden_one)\n",
    "            best_sae_two = copy.deepcopy(sae_hidden_two)\n",
    "    \n",
    "        validation_losses.append(avg_val_loss)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        SAE_hidden_one_losses.append(avg_sae_train_loss_hidden_one)\n",
    "        SAE_hidden_two_losses.append(avg_sae_train_loss_hidden_two)\n",
    "    \n",
    "    Z_train_one = feature_activations_one_train.detach().T.numpy()\n",
    "    Z_train_two = feature_activations_two_train.detach().T.numpy()\n",
    "    y_train = labels_train.numpy()\n",
    "    \n",
    "    \n",
    "    ######################################################################################################\n",
    "    # EVAL\n",
    "    ######################################################################################################\n",
    "    best_model.eval()  # again currently a no-op\n",
    "    best_sae_one.eval()\n",
    "    best_sae_two.eval()\n",
    "    \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    feature_activations_one_test = torch.zeros(HIDDEN_SIZE, len(test_images))\n",
    "    feature_activations_two_test = torch.zeros(HIDDEN_SIZE, len(test_images))\n",
    "    labels_test = torch.zeros(len(test_images), dtype=torch.long)\n",
    "    \n",
    "    recon_errors_one = []\n",
    "    recon_errors_two = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "        for i, batch in enumerate(test_bar):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "            # forward pass\n",
    "            classification_out, hidden_one_act, hidden_two_act = best_model(images)\n",
    "            reconstructed_one, encoded_one = best_sae_one(hidden_one_act)\n",
    "            reconstructed_two, encoded_two = best_sae_two(hidden_two_act)\n",
    "    \n",
    "            # SAE reconstruction\n",
    "            recon_errors_one.append(torch.mean((hidden_one_act - reconstructed_one) ** 2).item())\n",
    "            recon_errors_two.append(torch.mean((hidden_two_act - reconstructed_two) ** 2).item())\n",
    "    \n",
    "            start_idx = i * test_loader.batch_size\n",
    "            end_idx = start_idx + images.size(0)\n",
    "            feature_activations_one_test[:, start_idx:end_idx] = encoded_one.T.cpu()\n",
    "            feature_activations_two_test[:, start_idx:end_idx] = encoded_two.T.cpu()\n",
    "            labels_test[start_idx:end_idx] = labels.cpu()\n",
    "    \n",
    "            # stats\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    \n",
    "    Z_test_one = feature_activations_one_test.T.numpy()\n",
    "    Z_test_two = feature_activations_two_test.T.numpy()\n",
    "    y_test = labels_test.numpy()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # reconstruction accuracy of SAE at each layer\n",
    "    avg_recon_error_one = np.mean(recon_errors_one)\n",
    "    avg_recon_error_two = np.mean(recon_errors_two)\n",
    "    print(f\"Average Reconstruction Error (Hidden One): {avg_recon_error_one:.4f}\")\n",
    "    print(f\"Average Reconstruction Error (Hidden Two): {avg_recon_error_two:.4f}\")\n",
    "    \n",
    "    # Compute sparsity (average non-zero features per image)\n",
    "    sparsity_one = torch.mean((feature_activations_one_test > 1e-5).float()).item() * 64\n",
    "    sparsity_two = torch.mean((feature_activations_two_test > 1e-5).float()).item() * 64\n",
    "    print(f\"Average Non-Zero Features per Image (Hidden One): {sparsity_one:.2f}\")\n",
    "    print(f\"Average Non-Zero Features per Image (Hidden Two): {sparsity_two:.2f}\")\n",
    "    \n",
    "    ######################################################################################################\n",
    "    # SPARSE FEATURE PROBES\n",
    "    ######################################################################################################\n",
    "        \n",
    "    clf_one = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "    clf_one.fit(Z_train_one, y_train)\n",
    "    acc_one = clf_one.score(Z_test_one, y_test)\n",
    "    print(f\"Linear Probe Accuracy (Hidden One): {acc_one:.2%}\")\n",
    "    \n",
    "    clf_two = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "    clf_two.fit(Z_train_two, y_train)\n",
    "    acc_two = clf_two.score(Z_test_two, y_test)\n",
    "    print(f\"Linear Probe Accuracy (Hidden Two): {acc_two:.2%}\")\n",
    "\n",
    "    loss_data_dict[loss_factor] = {}\n",
    "    loss_data_dict[loss_factor][\"Final_Accuracy\"] = test_accuracy\n",
    "    loss_data_dict[loss_factor][\"Final_Loss\"] = avg_train_loss\n",
    "    loss_data_dict[loss_factor][\"S1_Probe_Acccuracy\"] = acc_one\n",
    "    loss_data_dict[loss_factor][\"S2_Probe_Acccuracy\"] = acc_two\n",
    "    loss_data_dict[loss_factor][\"Loss_Plot_Data\"] = (validation_losses, training_losses, SAE_hidden_one_losses, SAE_hidden_two_losses)\n",
    "\n",
    "    del feature_activations_one_train, feature_activations_two_train, labels_train\n",
    "    del feature_activations_one_test, feature_activations_two_test, labels_test\n",
    "    del best_model, best_sae_one, best_sae_two\n",
    "    del validation_losses, training_losses, SAE_hidden_one_losses, SAE_hidden_two_losses\n",
    "    del clf_two, clf_one\n",
    "    del model, sae_hidden_one, sae_hidden_two\n",
    "    del train_dataset, train_loader\n",
    "    del test_dataset, test_loader\n",
    "    del val_dataset, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce1426-747d-470a-a42a-d5b13b3c776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd96fe2-4d73-41b2-8d57-556ba65e9e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_acc = 0\n",
    "max_digit = -1\n",
    "for digit in loss_data_dict.keys():\n",
    "    if loss_data_dict[digit][\"Final_Accuracy\"] > max_acc:\n",
    "        max_acc = loss_data_dict[digit][\"Final_Accuracy\"]\n",
    "        max_digit = digit\n",
    "\n",
    "print(max_digit)\n",
    "print(loss_data_dict[max_digit][\"Final_Accuracy\"])\n",
    "print(loss_data_dict[max_digit][\"S1_Probe_Acccuracy\"])\n",
    "print(loss_data_dict[max_digit][\"S2_Probe_Acccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee6c71-6f71-40fc-9d21-2ccd3ea32b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"./loss_data_dict_{min_loss}_to_{max_loss}_mask_sparse.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(loss_data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f36f1-d980-4243-a39d-9097338e2bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
