{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b8ed11-7f55-4e12-a6ad-4cacf006ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 10 16:02:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.58.02              Driver Version: 555.58.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:0A:00.0 Off |                  Off |\n",
      "|  0%   39C    P8             15W /  450W |     524MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1445      G   /usr/lib/xorg/Xorg                            419MiB |\n",
      "|    0   N/A  N/A      1700      G   /usr/bin/gnome-shell                           53MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b2e640-7b6c-4b01-9b2b-4288474521f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "VisionTransformer                                  [1, 1000]                 1,280\n",
       "├─Conv2d: 1-1                                      [1, 1280, 16, 16]         753,920\n",
       "├─Encoder: 1-2                                     [1, 257, 1280]            328,960\n",
       "│    └─Dropout: 2-1                                [1, 257, 1280]            --\n",
       "│    └─Sequential: 2-2                             [1, 257, 1280]            --\n",
       "│    │    └─EncoderBlock: 3-1                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-2                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-3                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-4                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-5                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-6                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-7                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-8                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-9                      [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-10                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-11                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-12                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-13                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-14                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-15                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-16                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-17                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-18                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-19                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-20                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-21                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-22                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-23                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-24                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-25                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-26                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-27                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-28                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-29                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-30                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-31                     [1, 257, 1280]            19,677,440\n",
       "│    │    └─EncoderBlock: 3-32                     [1, 257, 1280]            19,677,440\n",
       "│    └─LayerNorm: 2-3                              [1, 257, 1280]            2,560\n",
       "├─Sequential: 1-3                                  [1, 1000]                 --\n",
       "│    └─Linear: 2-4                                 [1, 1000]                 1,281,000\n",
       "====================================================================================================\n",
       "Total params: 632,045,800\n",
       "Trainable params: 632,045,800\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 614.09\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 594.76\n",
       "Params size (MB): 1687.35\n",
       "Estimated Total Size (MB): 2282.71\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import vit_h_14, ViT_H_14_Weights\n",
    "from torchinfo import summary\n",
    "\n",
    "# Load your model (as you already do)\n",
    "weights = ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "model = vit_h_14(weights=weights)\n",
    "\n",
    "# --- Add this line to print the summary ---\n",
    "# You must provide an example input size for the model.\n",
    "# (batch_size, channels, height, width)\n",
    "input_size = (1, 3, 224, 224) \n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc2ca47-21ea-4678-adcf-fc6b16c2e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "starting_block = model.encoder.layers[0]\n",
    "print(starting_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4875b4a1-ffee-4569-8c84-986f07d5ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = len(model.encoder.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a00ed30-03d3-4d42-9274-c0a0c5b8a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "middle_block = model.encoder.layers[num_blocks // 2]\n",
    "print(middle_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb5246b-e33b-405a-b3d7-af2afbe69cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "final_block = model.encoder.layers[num_blocks - 1]\n",
    "print(final_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da54bd20-44f5-40a8-beb6-7594c613b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_12): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_13): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_14): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_15): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_16): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_17): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_18): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_19): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_20): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_21): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_22): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_23): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_24): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_25): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_26): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_27): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_28): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_29): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_30): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_31): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5b499-93b4-40a6-8faf-7874d60b05d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
