{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79e3a18-be53-45e0-ba3e-272c10884e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import vit_h_14, ViT_H_14_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from helpers.sae import SparseAutoencoder\n",
    "from helpers.helpers import extract_activations, SNE_plot_2d, load_intermediate_labels\n",
    "\n",
    "EMBEDS_PATH = './embeds/pos_embed_edge_384_99.56.pth'\n",
    "# VIT_PATH = './classifiers/baseline/vit_h_99.56.pth'\n",
    "# SAE_PATH = './sae_models/baseline-99.56/last_layer/sae_last_layer_l1_0.0002.pth'\n",
    "\n",
    "# VIT_PATH = './classifiers/F0/best_model_lf_0.01.pth'\n",
    "# SAE_PATH = './sae_models/F0/sae_last_layer_l1_0.0002.pth'\n",
    "\n",
    "VIT_PATH = './classifiers/F1/best_model_lf_0.01.pth'\n",
    "SAE_PATH = './sae_models/F1/sae_last_layer_l1_0.0002.pth'\n",
    "\n",
    "# VIT_PATH = './classifiers/F2/best_model_lf_0.3.pth'\n",
    "# SAE_PATH = './sae_models/F2/sae_last_layer_l1_0.0002.pth'\n",
    "\n",
    "IMG_RES = 384\n",
    "# FEATURE_DIM = 1280\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"We will be using device: {device}\")\n",
    "\n",
    "# eval_transform = transforms.Compose([\n",
    "#     transforms.Resize((IMG_RES, IMG_RES)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=eval_transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False,\n",
    "#                 pin_memory=True, num_workers=4)\n",
    "\n",
    "# print(\"Loading ViT\")\n",
    "# weights = ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "# model = vit_h_14(weights=weights) \n",
    "# model.image_size = IMG_RES  # Update the expected image size\n",
    "\n",
    "# model.encoder.pos_embedding = torch.nn.Parameter(torch.load(EMBEDS_PATH))\n",
    "\n",
    "# num_ftrs = model.heads.head.in_features\n",
    "# model.heads.head = torch.nn.Linear(num_ftrs, 10)\n",
    "# model.load_state_dict(torch.load(VIT_PATH))\n",
    "# model.to(device)\n",
    "\n",
    "# print(\"Loading SAE\")\n",
    "# sae = SparseAutoencoder(input_dim=FEATURE_DIM)\n",
    "# sae.load_state_dict(torch.load(SAE_PATH))\n",
    "# sae.to(device)\n",
    "\n",
    "# print(\"Extracting activations\")\n",
    "# activation_data = extract_activations(\n",
    "#     data_loader=test_loader,\n",
    "#     model=model,\n",
    "#     sae=sae,\n",
    "#     device=device\n",
    "# )\n",
    "seed = 42\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_RES, IMG_RES)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_RES, IMG_RES)),  # Ensure 384x384 for validation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "val_dataset_for_split = datasets.CIFAR10(root='./data', train=True, download=True, transform=eval_transform)\n",
    "\n",
    "num_train = len(full_train_dataset)\n",
    "split = int(0.9 * num_train)\n",
    "indices = torch.randperm(num_train, generator=generator).tolist()\n",
    "\n",
    "train_subset = torch.utils.data.Subset(full_train_dataset, indices[:split])\n",
    "\n",
    "\n",
    "RECON_ACT_BASE_PATH = \"./features/classifier-99.56/baseline\"\n",
    "N = 25\n",
    "sparse_type = \"top\"\n",
    "recon_act_path = f\"{RECON_ACT_BASE_PATH}/{N}_{sparse_type}.pkl\"\n",
    "recon_act_raw = load_intermediate_labels(recon_act_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91b8a3c-7896-466d-ab47-a2ecdceddc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for img, label in train_subset:\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5fd7aa-0c8b-443f-8f93-690f2c3d361c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caeb9a49-5078-4492-8435-98960b76311d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_act_raw[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c60a519-1d91-4ef3-8834-3fb91f4a2476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "output_path = \"./labels.pkl\"\n",
    "\n",
    "# 2. Open the file in \"write binary\" mode and save the object\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437ee2d-accb-40cc-8f3f-5a0cd8fc5b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
