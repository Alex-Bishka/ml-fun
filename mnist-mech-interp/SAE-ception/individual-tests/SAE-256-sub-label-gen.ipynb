{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84207c44-9c22-4853-943c-5a6cb08bdbdf",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "\n",
    "Creating the sub labels from our SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa37d0-59c1-4368-aec0-83ccb0ce6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EXPERIMENT_TYPE = \"SAE\"\n",
    "RUN_ID = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b67228-330c-480e-99b5-cb4001e89013",
   "metadata": {},
   "source": [
    "## SAE Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d7778-885a-4a6d-9bd2-ddbcb9a27be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "L1_PENALTY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4a0ba-710d-4882-9bd0-f948e9f6fb9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4018f1-cb83-4f0b-90db-0f48aad1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7038c3-06d1-4d3c-9b44-2f2bc3485f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_helpers import (plot_weights,\n",
    "                    plot_activations,\n",
    "                    plot_losses,\n",
    "                    plot_saliency_map,\n",
    "                    plot_sparse_vecs_by_image,\n",
    "                    plot_top_act_images_by_feature,\n",
    "                    feature_inversion\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c069cc3-458e-4825-9828-c771c7f7370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# assume cwd is project_root/data_loader\n",
    "project_root = Path(os.getcwd()).parent  # go up one level to project_root\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers import load_images, load_labels, visualize_image, get_edges, save_intermediate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef66c-0819-4e4f-9fb7-346b8b786a6b",
   "metadata": {},
   "source": [
    "## Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93931-772a-4720-b394-0cff7884a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We will be using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b553d6d-7e3a-4973-a472-3c735df1b699",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070db857-3892-4a37-b24a-b3f8a8d7bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_images = load_images(\"../data/train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "train_labels = load_labels(\"../data/train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels,\n",
    "    test_size=1/6,  # 10k validation\n",
    "    stratify=train_labels,\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_images = load_images(\"../data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels = load_labels(\"../data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e03f6-1174-4b0a-bac4-a30b30deff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Val images shape:\", val_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca05d5-d791-4fbb-9736-b52a5eae1409",
   "metadata": {},
   "source": [
    "## Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f14c-5f30-4237-933c-55a6f254a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = train_images[0]\n",
    "sample_label = train_labels[0]\n",
    "visualize_image(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd35-fd62-4598-89f5-58f98567ad3c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "### NN\n",
    "\n",
    "Once again, two hidden layers. 16 nodes each. Same as 3blue1brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d2535-98f4-4c1e-8073-c5373a5b56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_size_by_pixels = 28*28\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # define layers separately to have access to each\n",
    "        self.hidden_one = nn.Linear(layer_size_by_pixels, 16)\n",
    "        self.hidden_two = nn.Linear(16, 16)\n",
    "        self.classification_layer = nn.Linear(16, 10)\n",
    "        \n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # first hidden layer\n",
    "        hidden_one_out = self.hidden_one(x)\n",
    "        hidden_one_act = self.activation_function(hidden_one_out)\n",
    "\n",
    "        # second hidden layer\n",
    "        hidden_two_out = self.hidden_two(hidden_one_act)\n",
    "        hidden_two_act = self.activation_function(hidden_two_out)\n",
    "\n",
    "        # classification layer\n",
    "        classification_out = self.classification_layer(hidden_two_act)\n",
    "        \n",
    "        return classification_out, hidden_one_act, hidden_two_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff754-354f-4345-b7a8-637e1cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9cc14-5e72-4f33-b2cc-cd356e881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e8694-36ec-4f8b-b78d-f63acdc38dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model weights (to compare below): {model.hidden_one.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44c3cf-602d-451e-b10c-8b7f005858e8",
   "metadata": {},
   "source": [
    "### SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53786553-7c2c-4d0f-96a9-b901297c7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=16, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.activation(self.encoder(x))\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed, encoded\n",
    "    \n",
    "    def loss(self, x, reconstructed, encoded, l1_lambda=0.001):\n",
    "        mse_loss = nn.MSELoss()(reconstructed, x)\n",
    "        l1_loss = l1_lambda * torch.mean(torch.abs(encoded))\n",
    "        return mse_loss + l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba9c08-a5f4-47c2-b82a-3ea1bb8a41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "sae_hidden_two = SparseAutoencoder(input_size=16, hidden_size=HIDDEN_SIZE).to(device)\n",
    "optimizer_sae_hidden_two = torch.optim.Adam(sae_hidden_two.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035a794-7e07-4801-b8f0-0661db1e596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expectation: SAE weights: [ 0.19113463  0.20750198 -0.05856812  0.22965282 -0.05477589]\n",
    "print(f\"SAE weights: {sae_hidden_two.encoder.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea630df-97fa-4b7e-8f6b-10980cf3eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "sae_hidden_one = SparseAutoencoder(input_size=16, hidden_size=HIDDEN_SIZE).to(device)\n",
    "optimizer_sae_hidden_one = torch.optim.Adam(sae_hidden_one.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d0018-daf9-4515-8c93-9432dcb740d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expectation: SAE weights: [ 0.19113463  0.20750198 -0.05856812  0.22965282 -0.05477589]\n",
    "print(f\"SAE weights: {sae_hidden_one.encoder.weight[0][:5].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a92-a504-453b-96b3-6c0827fcb344",
   "metadata": {},
   "source": [
    "### Verify Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e28ddd-9831-421e-b1d1-767c0f837e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "model_compare_one = NeuralNetwork().to(device)\n",
    "first_set_of_weights = model_compare_one.hidden_one.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"First set of weights:\", first_set_of_weights)\n",
    "\n",
    "# reset the seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model_compare_two = NeuralNetwork().to(device)\n",
    "second_set_of_weights = model_compare_two.hidden_one.weight[0][:5].detach().cpu().numpy()\n",
    "print(\"Second set of weights:\", second_set_of_weights)\n",
    "\n",
    "print(f\"Are the two sets equal: {first_set_of_weights == second_set_of_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3f1a-32f7-46d9-975d-3c7c77d95a9f",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0386b-042b-4e96-8266-adb647758c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.images[idx].copy()).float(),\n",
    "            torch.tensor(self.labels[idx].copy(), dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f8215-1f78-4e3f-975f-1bddf935cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility on training\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c300da9-178d-46a0-ba00-cf68f0e7cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "if device.type.lower() == \"cpu\":\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "# training data\n",
    "train_dataset = EdgeDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=NUM_WORKERS, worker_init_fn=seed_worker, generator=generator)\n",
    "\n",
    "# validation data\n",
    "val_dataset = EdgeDataset(val_images, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)  # larger batch size for faster validation\n",
    "\n",
    "# test data\n",
    "test_dataset = EdgeDataset(test_images, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878bdc7-4dc6-4510-a3a7-d64a2d2d7394",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23049d-f81e-4a4b-92e6-3ca4ee276d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_sae_one = None\n",
    "best_sae_two = None\n",
    "num_epochs = 20\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "SAE_hidden_one_losses = []\n",
    "SAE_hidden_two_losses = []\n",
    "\n",
    "# Initialize storage for training features and labels\n",
    "feature_activations_one_train = torch.zeros(HIDDEN_SIZE, len(train_loader.dataset))\n",
    "feature_activations_two_train = torch.zeros(HIDDEN_SIZE, len(train_loader.dataset))\n",
    "labels_train = torch.zeros(len(train_loader.dataset), dtype=torch.long)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # set the model to training mode - this is currently a no-op\n",
    "    sae_hidden_two.train()\n",
    "    sae_hidden_one.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    total_sae_loss_hidden_two = 0.0\n",
    "    total_sae_loss_hidden_one = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False)\n",
    "    for batch_idx, batch in enumerate(train_bar):\n",
    "        # deconstruct batch items\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        classification_out, hidden_act_one, hidden_act_two = model(images)\n",
    "\n",
    "        # Classification loss and backprop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = classification_loss_fn(classification_out, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += total_loss.item()\n",
    "        train_bar.set_postfix(loss=total_loss.item())\n",
    "\n",
    "        # to prevent backprop on both graphs:\n",
    "        hidden_act_one_detached = hidden_act_one.detach()\n",
    "        hidden_act_two_detached = hidden_act_two.detach()\n",
    "\n",
    "        # SAE loss and backprop - hidden layer one\n",
    "        optimizer_sae_hidden_one.zero_grad()\n",
    "        reconstructed_one, encoded_one = sae_hidden_one(hidden_act_one_detached)\n",
    "        sae_loss_hidden_one = sae_hidden_one.loss(hidden_act_one_detached,\n",
    "                                                  reconstructed_one,\n",
    "                                                  encoded_one,\n",
    "                                                  l1_lambda=L1_PENALTY\n",
    "                                                 )\n",
    "        sae_loss_hidden_one.backward()\n",
    "        optimizer_sae_hidden_one.step()\n",
    "        total_sae_loss_hidden_one += sae_loss_hidden_one.item()\n",
    "        \n",
    "        # SAE loss and backprop - hidden layer two\n",
    "        optimizer_sae_hidden_two.zero_grad()\n",
    "        reconstructed_two, encoded_two = sae_hidden_two(hidden_act_two_detached)\n",
    "        sae_loss_hidden_two = sae_hidden_two.loss(hidden_act_two_detached,\n",
    "                                                  reconstructed_two,\n",
    "                                                  encoded_two,\n",
    "                                                  l1_lambda=L1_PENALTY\n",
    "                                                 )\n",
    "        sae_loss_hidden_two.backward()\n",
    "        optimizer_sae_hidden_two.step()\n",
    "        total_sae_loss_hidden_two += sae_loss_hidden_two.item()\n",
    "\n",
    "        # Store training features and labels\n",
    "        start_idx = batch_idx * train_loader.batch_size\n",
    "        end_idx = start_idx + images.size(0)\n",
    "        feature_activations_one_train[:, start_idx:end_idx] = encoded_one.T.cpu()\n",
    "        feature_activations_two_train[:, start_idx:end_idx] = encoded_two.T.cpu()\n",
    "        labels_train[start_idx:end_idx] = labels.cpu()\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            # deconstruct\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            classification_out, _, _ = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = classification_loss_fn(classification_out, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(classification_out, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # epoch stats\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_sae_train_loss_hidden_one = total_sae_loss_hidden_one / len(train_loader)\n",
    "    avg_sae_train_loss_hidden_two = total_sae_loss_hidden_two / len(train_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  SAE Train Loss (hidden one): {avg_sae_train_loss_hidden_one:.4f}\")\n",
    "    print(f\"  SAE Train Loss (hidden two): {avg_sae_train_loss_hidden_two:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    model_path = f'./models/{EXPERIMENT_TYPE}/{RUN_ID}/best_model_baseline_{epoch+1}.pth'\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_val_loss = avg_val_loss  # Update loss for reference\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"  Saved model with Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_sae_one = copy.deepcopy(sae_hidden_one)\n",
    "        best_sae_two = copy.deepcopy(sae_hidden_two)\n",
    "        \n",
    "    # Optional: Save if accuracy is equal but loss is lower\n",
    "    elif val_accuracy == best_val_acc and avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"  Saved model with same Val Acc: {val_accuracy:.2f}% but lower Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_sae_one = copy.deepcopy(sae_hidden_one)\n",
    "        best_sae_two = copy.deepcopy(sae_hidden_two)\n",
    "\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    SAE_hidden_one_losses.append(avg_sae_train_loss_hidden_one)\n",
    "    SAE_hidden_two_losses.append(avg_sae_train_loss_hidden_two)\n",
    "\n",
    "    plot_weights(model, epoch, EXPERIMENT_TYPE, RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16368999-b90c-4d90-9791-5341d039e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train_one = feature_activations_one_train.detach().T.numpy()\n",
    "Z_train_two = feature_activations_two_train.detach().T.numpy()\n",
    "y_train = labels_train.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4aa2ac-a39c-4714-bb36-094a5267a5d5",
   "metadata": {},
   "source": [
    "# Training Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06952990-a3a5-44f3-9325-21f1de91dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(training_losses, validation_losses, label_one=\"Training\", label_two=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcc9ad-9de1-484e-9df1-c10058973614",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(SAE_hidden_one_losses, SAE_hidden_two_losses, \n",
    "            label_one=\"SAE Hidden One\", label_two=\"SAE Hidden Two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3ea2-fd60-4e54-8942-33172e4de452",
   "metadata": {},
   "source": [
    "## Eval\n",
    "\n",
    "Evaluating our classification model accuracy, in addition to the reconstruction and sparisty metrics of each SAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79dc794-b35a-4ed7-bc9b-61c52ca7e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()  # again currently a no-op\n",
    "best_sae_one.eval()\n",
    "best_sae_two.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "feature_activations_one_test = torch.zeros(HIDDEN_SIZE, len(test_images))\n",
    "feature_activations_two_test = torch.zeros(HIDDEN_SIZE, len(test_images))\n",
    "labels_test = torch.zeros(len(test_images), dtype=torch.long)\n",
    "\n",
    "recon_errors_one = []\n",
    "recon_errors_two = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=f\"Evaluation\")\n",
    "    for i, batch in enumerate(test_bar):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        classification_out, hidden_one_act, hidden_two_act = best_model(images)\n",
    "        reconstructed_one, encoded_one = best_sae_one(hidden_one_act)\n",
    "        reconstructed_two, encoded_two = best_sae_two(hidden_two_act)\n",
    "\n",
    "        # SAE reconstruction\n",
    "        recon_errors_one.append(torch.mean((hidden_one_act - reconstructed_one) ** 2).item())\n",
    "        recon_errors_two.append(torch.mean((hidden_two_act - reconstructed_two) ** 2).item())\n",
    "\n",
    "        start_idx = i * test_loader.batch_size\n",
    "        end_idx = start_idx + images.size(0)\n",
    "        feature_activations_one_test[:, start_idx:end_idx] = encoded_one.T.cpu()\n",
    "        feature_activations_two_test[:, start_idx:end_idx] = encoded_two.T.cpu()\n",
    "        labels_test[start_idx:end_idx] = labels.cpu()\n",
    "\n",
    "        # stats\n",
    "        _, predicted = torch.max(classification_out, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "Z_test_one = feature_activations_one_test.T.numpy()\n",
    "Z_test_two = feature_activations_two_test.T.numpy()\n",
    "y_test = labels_test.numpy()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# reconstruction accuracy of SAE at each layer\n",
    "avg_recon_error_one = np.mean(recon_errors_one)\n",
    "avg_recon_error_two = np.mean(recon_errors_two)\n",
    "print(f\"Average Reconstruction Error (Hidden One): {avg_recon_error_one:.4f}\")\n",
    "print(f\"Average Reconstruction Error (Hidden Two): {avg_recon_error_two:.4f}\")\n",
    "\n",
    "# Compute sparsity (average non-zero features per image)\n",
    "sparsity_one = torch.mean((feature_activations_one_test > 1e-5).float()).item() * 64\n",
    "sparsity_two = torch.mean((feature_activations_two_test > 1e-5).float()).item() * 64\n",
    "print(f\"Average Non-Zero Features per Image (Hidden One): {sparsity_one:.2f}\")\n",
    "print(f\"Average Non-Zero Features per Image (Hidden Two): {sparsity_two:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afece9-1762-4de9-b70a-6465ef6ffa3c",
   "metadata": {},
   "source": [
    "## Linear Probe Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcbcfb-904d-486e-8890-d624cf3c74ba",
   "metadata": {},
   "source": [
    "### Exploring First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819dd92-86a4-41c4-a584-22ca7f3138d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_one = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "clf_one.fit(Z_train_one, y_train)\n",
    "acc_one = clf_one.score(Z_test_one, y_test)\n",
    "print(f\"Linear Probe Accuracy (Hidden One): {acc_one:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75661f9-7460-49b6-a6d6-834f938acc8e",
   "metadata": {},
   "source": [
    "### Exploring Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201dc0c-dd79-49fd-8aad-f93e73e2510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_two = LogisticRegression(penalty='l2', max_iter=1000, n_jobs=-1)\n",
    "clf_two.fit(Z_train_two, y_train)\n",
    "acc_two = clf_two.score(Z_test_two, y_test)\n",
    "print(f\"Linear Probe Accuracy (Hidden Two): {acc_two:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664aa23d-5d37-47f1-9b97-ec0dce089d91",
   "metadata": {},
   "source": [
    "# Obtaining Reconstruction and Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adf290-7f2e-4dea-a968-9fe3cae7dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activations_one = []\n",
    "hidden_activations_two = []\n",
    "\n",
    "sparse_act_one = []\n",
    "sparse_act_two = []\n",
    "\n",
    "recon_act_one = []\n",
    "recon_act_two = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(train_labels)):\n",
    "        image = torch.from_numpy(train_images[i]).float().unsqueeze(0).to(device)\n",
    "        label = train_labels[i]\n",
    "\n",
    "        # Forward pass\n",
    "        classification_out, hidden_one_act, hidden_two_act = best_model(image)\n",
    "        recon, encoding = best_sae_one(hidden_one_act)\n",
    "        recon_two, encoding_two = best_sae_two(hidden_two_act)\n",
    "        \n",
    "        # Get predicted class\n",
    "        hidden_activations_one.append(hidden_one_act.cpu().numpy().flatten())\n",
    "        hidden_activations_two.append(hidden_two_act.cpu().numpy().flatten())\n",
    "\n",
    "        sparse_act_one.append(encoding.cpu().numpy().flatten())\n",
    "        sparse_act_two.append(encoding_two.cpu().numpy().flatten())\n",
    "\n",
    "        recon_act_one.append(recon.cpu().numpy().flatten())\n",
    "        recon_act_two.append(recon_two.cpu().numpy().flatten())\n",
    "        \n",
    "        labels.append(label)\n",
    "\n",
    "hidden_activations_one = np.array(hidden_activations_one)\n",
    "hidden_activations_two = np.array(hidden_activations_two)\n",
    "\n",
    "sparse_act_one = np.array(sparse_act_one)\n",
    "sparse_act_two = np.array(sparse_act_two)\n",
    "\n",
    "recon_act_one = np.array(recon_act_one)\n",
    "recon_act_two = np.array(recon_act_two)\n",
    "\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4b95c-f7c7-45a4-a9d8-7b54aa2157dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_N_features(N, sparse_act_one, labels):\n",
    "    avg_digit_encoding = {}\n",
    "    top_n_features = {}\n",
    "    \n",
    "    for digit in range(10):\n",
    "        mask = labels == digit\n",
    "        mean_digit_encoding = np.mean(sparse_act_one[mask], axis=0)\n",
    "        avg_digit_encoding[digit] = mean_digit_encoding\n",
    "    \n",
    "        top_n_indices = np.argsort(mean_digit_encoding)[-N:][::-1]  # Sort descending\n",
    "        top_n_values = mean_digit_encoding[top_n_indices]\n",
    "    \n",
    "        top_n_features[digit] = {\n",
    "            'indices': top_n_indices.tolist(),\n",
    "            'activations': top_n_values.tolist()\n",
    "        }\n",
    "    \n",
    "    return avg_digit_encoding, top_n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c943f-deac-4cd1-95ab-9f1503188be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = HIDDEN_SIZE\n",
    "avg_digit_encoding, top_n_features = get_top_N_features(N, sparse_act_one, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089cf16-f72c-4a03-b281-f39ad45dec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices_dict = {}\n",
    "for digit in range(0, 10):\n",
    "    feature_indices_dict[digit] = top_n_features[digit]['indices']\n",
    "\n",
    "# feature_indices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc254f-b494-4131-b58d-b99c3f2b7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_indices_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9f4cd-907e-4b90-841d-b176221fbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sublabel_data(data_labels, data_images, feature_indices_dict, sparse_activations, sae_model):\n",
    "    recon_max_sparse = []\n",
    "    recon_max_sparse_ablated = []\n",
    "    for i, label in enumerate(data_labels):\n",
    "        feature_indices = feature_indices_dict[label]\n",
    "        image = torch.from_numpy(data_images[i]).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        sparse_vector = sparse_activations[i].copy()\n",
    "        encoded = np.zeros(HIDDEN_SIZE)\n",
    "        encoded_ablated = sparse_vector.copy()\n",
    "        \n",
    "        for feature_idx in feature_indices:\n",
    "            encoded[feature_idx] = sparse_vector[feature_idx]\n",
    "            encoded_ablated[feature_idx] = 0\n",
    "        \n",
    "        encoded_digit_torch = torch.from_numpy(encoded).float().to(device).unsqueeze(0)\n",
    "        encoded_digit_torch_ablated = torch.from_numpy(encoded_ablated).float().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            recon = sae_model.decoder(encoded_digit_torch)\n",
    "            recon = recon.view(1, -1)\n",
    "    \n",
    "            recon_ablated = best_sae_one.decoder(encoded_digit_torch_ablated)\n",
    "            recon_ablated = recon.view(1, -1)\n",
    "    \n",
    "            recon_max_sparse.append(recon.cpu().detach().clone())\n",
    "            recon_max_sparse_ablated.append(recon_ablated.cpu().detach().clone())\n",
    "\n",
    "    return recon_max_sparse, recon_max_sparse_ablated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe995e-26d6-48c9-a888-59313b062d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_max_sparse_training, recon_max_sparse_ablated_training = get_sublabel_data(train_labels,\n",
    "                                                                                 train_images,\n",
    "                                                                                 feature_indices_dict,\n",
    "                                                                                 sparse_act_one,\n",
    "                                                                                 best_sae_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722746fd-e58e-4c3d-9051-571b48173158",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(val_images), len(test_images), len(recon_max_sparse_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167b668-4327-450d-8642-f934c14b878d",
   "metadata": {},
   "source": [
    "# Saving Intermediate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3825631-ff07-4da5-a328-c948bb1b746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612e21f-fa14-4a3a-85b7-d07a00123857",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"./intermediate-labels/first_layer/train_images_recon_full_sparse.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(recon_max_sparse_training, f)\n",
    "\n",
    "file_path = f\"./intermediate-labels/first_layer/train_images_recon_0_sparse.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(recon_max_sparse_ablated_training, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d46d94-cef5-41aa-9806-fb427308ab3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
