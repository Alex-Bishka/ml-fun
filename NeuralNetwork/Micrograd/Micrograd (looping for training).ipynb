{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b22c61",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4d5304-e56c-44eb-af06-8ccec4f6aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from value import Value\n",
    "from graph_visualization_code import draw_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddeece3-67f8-4a68-bd6f-77dcb0262f1c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c8e1f0-aaf1-499c-b748-fb822717d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        \"\"\"\n",
    "        nin: number of inputs\n",
    "        \"\"\"\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)] # random weight b/w -1 and 1 for every input\n",
    "        self.b = Value(random.uniform(-1, 1)) # the bias, which controls the overall trigger happiness of the neuron\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        What we want to do here is the weighted sum, including the bias: w * x + b\n",
    "\n",
    "        In other words, the dot product of w and x to get the forward pass of the neuron\n",
    "\n",
    "        What we need to do here:\n",
    "            1. Multiply all the elements of w, with all of the elements of x, pairwise\n",
    "            2. Add the bias to the weighted sum\n",
    "        \"\"\"\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        PyTorch has a parameters call on every single module\n",
    "        \"\"\"\n",
    "        return self.w + [self.b] # returns concatenation of the weights and biases\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    A list of neurons\n",
    "\n",
    "    nin: number of inputs for the neuron in the layer\n",
    "    nout: how many neurons we will have in a layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs # to return just the final output value, instead of it being wrapped in a list\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        \n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"\n",
    "        nin: number of inputs (as before)\n",
    "        nouts: number of outputs (which is the neurons in a single layer) is now a list - this list defines the sizes of all the layers in the MLP\n",
    "        \"\"\"\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a897fa-3769-4fc5-a1c6-16e79ab9b3d6",
   "metadata": {},
   "source": [
    "The network structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f97871-4a27-4b0a-a8ec-652bb2a0ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.5978655363471771)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]   # three inputs into the MLP\n",
    "n = MLP(3, [4, 4, 1])  # 3 layers of size 4, 4, and 1 - the last being the output\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b8696-9062-4208-95ed-717ab428e21c",
   "metadata": {},
   "source": [
    "The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9cb2a3-89af-469e-b193-5434620d9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0], # so desired output is 1.0\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0506950-036b-4466-9537-984323c6d574",
   "metadata": {},
   "source": [
    "Our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398ec287-43cc-4e4d-aa65-54459e72b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.226138135169745\n",
      "1 1.9463119241537454\n",
      "2 1.1348461224920454\n",
      "3 0.6575899537017964\n",
      "4 0.4081322564768234\n",
      "5 0.2815404514170492\n",
      "6 0.21012713395765253\n",
      "7 0.16472114190415132\n",
      "8 0.13384088388732432\n",
      "9 0.11176560464612696\n",
      "10 0.09535826438928993\n",
      "11 0.08277734820717694\n",
      "12 0.07288099761774844\n",
      "13 0.06492883626834328\n",
      "14 0.05842268887336329\n",
      "15 0.05301672222853071\n",
      "16 0.04846436880990715\n",
      "17 0.044585728915217775\n",
      "18 0.04124686742831188\n",
      "19 0.038346276450447726\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(y_output - y_ground_truth)**2 for y_ground_truth, y_output in zip(ys, ypred)])\n",
    "\n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters - gradient descent\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "\n",
    "    # print step and loss value\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1caade1-bc14-4b16-810e-24fd0c3e440c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9008488908408923),\n",
       " Value(data=-0.8850568105053157),\n",
       " Value(data=-0.918501485522933),\n",
       " Value(data=0.9069334145367586)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64980e07-8114-459e-aac7-b49aac9328bb",
   "metadata": {},
   "source": [
    "Neural nets can be tricky. They can work in spite of having bugs in the code (like forgetting to flush gradients)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec7496-3115-44f8-b810-8474033b130c",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09d705-2238-40cb-b7c6-130698e5137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left off at 2:14:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
