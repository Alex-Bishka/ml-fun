{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b22c61",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4d5304-e56c-44eb-af06-8ccec4f6aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from value import Value\n",
    "from graph_visualization_code import draw_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddeece3-67f8-4a68-bd6f-77dcb0262f1c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c8e1f0-aaf1-499c-b748-fb822717d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        \"\"\"\n",
    "        nin: number of inputs\n",
    "        \"\"\"\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)] # random weight b/w -1 and 1 for every input\n",
    "        self.b = Value(random.uniform(-1, 1)) # the bias, which controls the overall trigger happiness of the neuron\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        What we want to do here is the weighted sum, including the bias: w * x + b\n",
    "\n",
    "        In other words, the dot product of w and x to get the forward pass of the neuron\n",
    "\n",
    "        What we need to do here:\n",
    "            1. Multiply all the elements of w, with all of the elements of x, pairwise\n",
    "            2. Add the bias to the weighted sum\n",
    "        \"\"\"\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        PyTorch has a parameters call on every single module\n",
    "        \"\"\"\n",
    "        return self.w + [self.b] # returns concatenation of the weights and biases\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    A list of neurons\n",
    "\n",
    "    nin: number of inputs for the neuron in the layer\n",
    "    nout: how many neurons we will have in a layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs # to return just the final output value, instead of it being wrapped in a list\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        \n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"\n",
    "        nin: number of inputs (as before)\n",
    "        nouts: number of outputs (which is the neurons in a single layer) is now a list - this list defines the sizes of all the layers in the MLP\n",
    "        \"\"\"\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a897fa-3769-4fc5-a1c6-16e79ab9b3d6",
   "metadata": {},
   "source": [
    "The network structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f97871-4a27-4b0a-a8ec-652bb2a0ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.5978655363471771)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]   # three inputs into the MLP\n",
    "n = MLP(3, [4, 4, 1])  # 3 layers of size 4, 4, and 1 - the last being the output\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b8696-9062-4208-95ed-717ab428e21c",
   "metadata": {},
   "source": [
    "The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9cb2a3-89af-469e-b193-5434620d9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0], # so desired output is 1.0\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0506950-036b-4466-9537-984323c6d574",
   "metadata": {},
   "source": [
    "Our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398ec287-43cc-4e4d-aa65-54459e72b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.226138135169745\n",
      "1 1.9463119241537454\n",
      "2 1.1348461224920454\n",
      "3 0.6575899537017964\n",
      "4 0.4081322564768234\n",
      "5 0.2815404514170492\n",
      "6 0.21012713395765253\n",
      "7 0.16472114190415132\n",
      "8 0.13384088388732432\n",
      "9 0.11176560464612696\n",
      "10 0.09535826438928993\n",
      "11 0.08277734820717694\n",
      "12 0.07288099761774844\n",
      "13 0.06492883626834328\n",
      "14 0.05842268887336329\n",
      "15 0.05301672222853071\n",
      "16 0.04846436880990715\n",
      "17 0.044585728915217775\n",
      "18 0.04124686742831188\n",
      "19 0.038346276450447726\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(y_output - y_ground_truth)**2 for y_ground_truth, y_output in zip(ys, ypred)])\n",
    "\n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters - gradient descent\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "\n",
    "    # print step and loss value\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1caade1-bc14-4b16-810e-24fd0c3e440c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9008488908408923),\n",
       " Value(data=-0.8850568105053157),\n",
       " Value(data=-0.918501485522933),\n",
       " Value(data=0.9069334145367586)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64980e07-8114-459e-aac7-b49aac9328bb",
   "metadata": {},
   "source": [
    "Neural nets can be tricky. They can work in spite of having bugs in the code (like forgetting to flush gradients)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec7496-3115-44f8-b810-8474033b130c",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afc737-83ec-4063-80fe-56861b900c4f",
   "metadata": {},
   "source": [
    "Let's bring everything together and summarize what we learned.\n",
    "\n",
    "What are neural nets? Neural nets are these mathematical expressions (fairly simple in the case of an MLP), that take input as the data, along with weights and other parameters of the neural net. These allow you to calculate a final answer via the forward pass, which is simple weighted sums and applications of squishification functions. \n",
    "\n",
    "The loss function tries to measure the accuracy of the prediciton. This is low when the predictions match the target, i.e. when the network is behaving well. Then, we backward the loss. Back prop provides us the gradient, which tells us how to tune the parameters s.t. that can decrease the loss locally. This process must be iterated many times to improve the loss - this is called gradient descent. Following the gradient information will minimize the loss.\n",
    "\n",
    "We can make nets do arbitrary things. This can solve extremely complex problems. As neural nets scale up and attempt hard problems, they have emergent properties. These larger networks work on these same fundamental principles!\n",
    "\n",
    "In production, the gradient descent technique would likely differ. It probably wouldn't be a form of stochastic gradient descent. Additionally, the loss function is a little more complicated, cross entropy loss is typically employed over MSE.\n",
    "\n",
    "Also, when dataset increase drastically in size, batching is frequently used. Essentially this is picking a random subset of the training data to perform a training loop on. Learning rate decay is also common (start aggressive with the learning rate and decrease it over time to fine tune stepping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3749a5-7a6b-4362-a607-d31c19336585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
