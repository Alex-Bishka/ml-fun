{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fea2a1-331f-4551-864b-0dbd0c53e637",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Now it's time to do the exact same thing using a modern deep neural network library, like `PyTorch`. Micrograd is roughly modeled to this. \n",
    "\n",
    "So, we'll be doing the exact same thing but using the `PyTorch` API - a production grade package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b22c61",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4d5304-e56c-44eb-af06-8ccec4f6aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from value import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc5fa0-704a-4a21-96c2-fc0762ae7464",
   "metadata": {},
   "source": [
    "# PyTorch Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f279e2-3b28-4d06-ab03-01d92e759cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "----\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor([2.0]).double()  ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()  ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()  ; w2.requires_grad = True\n",
    "\n",
    "b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n",
    "\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print(\"----\")\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48faaca5-4a7f-4cb8-b3e5-b8b745e68c0a",
   "metadata": {},
   "source": [
    "Micrograd is a scalar value engine. PyTorch is based on tensors (which are n-dimensional arrays of scalars). In our case, we are using just a scalar valued tensor (a tensor with only one element in it). Our scalar is cast to double (by default the values are float 32, so this gives us float 64 types).\n",
    "\n",
    "`o` is a tensor object. Like Micrograd, PyTorch's tensor objects have `grad` and `data` attributes. The only difference here is that we need to call `.item()`, in order to grab the element from a position in the tensor (we are grabbing the specific element and stripping out the tensor).\n",
    "\n",
    "Note by default PyTorch sets the `requires_grad` prop to `False` for efficiency reasons. You typically don't want gradients for leaf nodes (like inputs in the network).\n",
    "\n",
    "PyTorch can do the same as Micrograd whent the tensors are single valued elements.\n",
    "\n",
    "The big deal with PyTorch is that everything is significantly more efficient, because we are working with tensors. We can do a lot of operations in parallel with these tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58628f-5f0f-42c3-877c-24bb71bd6c7a",
   "metadata": {},
   "source": [
    "## Tensor Objects\n",
    "\n",
    "Normally, you would use more complicated tensors like the following 2-by-3 array of scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8374894-9190-436b-a924-47d8f298683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4da9fe-410c-4b82-8a53-efbcd7cd91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[1, 2, 3], [4, 5, 6]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca3e48-aa51-4318-8657-1940ecbe2e64",
   "metadata": {},
   "source": [
    "This is usually what you would work with in the actual libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79dc57-92d7-4aeb-b9d0-7b42cb80c14d",
   "metadata": {},
   "source": [
    "What our output tensor looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a49bb2-89bd-4ff8-9106-ea9fc54a1f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7071], dtype=torch.float64, grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3848ef7e-ec9e-4d5a-ab3e-1c33c8332258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071066904050358"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.item() # which is the same here as o.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f62d90c-a647-4826-bc4b-bec83a436358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000], dtype=torch.float64)\n",
      "0.5000001283844369\n"
     ]
    }
   ],
   "source": [
    "print(x2.grad)\n",
    "print(x2.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582c40d-f44a-45e6-a616-cfcc334396ed",
   "metadata": {},
   "source": [
    "# Building Neural Nets\n",
    "\n",
    "Now that we have the mathematical machinery, we can build out neural nets. Neural nets are just a specific class of mathematical expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdfd1e-d89f-46e6-8738-32d2e53ffa90",
   "metadata": {},
   "source": [
    "## A Single Neuron\n",
    "\n",
    "Let's create a neuron that subscribes to the PyTorch API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a704978-35c1-457c-9499-c00c6eb2a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        \"\"\"\n",
    "        nin: number of inputs\n",
    "        \"\"\"\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)] # random weight b/w -1 and 1 for every input\n",
    "        self.b = Value(random.uniform(-1, 1)) # the bias, which controls the overall trigger happiness of the neuron\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        What we want to do here is the weighted sum, including the bias: w * x + b\n",
    "\n",
    "        In other words, the dot product of w and x to get the forward pass of the neuron\n",
    "\n",
    "        What we need to do here:\n",
    "            1. Multiply all the elements of w, with all of the elements of x, pairwise\n",
    "            2. Add the bias to the weighted sum\n",
    "        \"\"\"\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d136b8b-ded4-459c-b6d8-024e14c57b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9825110257057832)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb30eb-e72e-4c28-8612-dd035d407d79",
   "metadata": {},
   "source": [
    "Now we can forward a single neuron!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962ebf2-db40-45aa-b865-60e556be4366",
   "metadata": {},
   "source": [
    "## Defining a Layer\n",
    "\n",
    "Here we will define a layer of neurons. Each layer has a number of neurons. They are not connected to each other. All of the neurons are fully connected to the \"input\". A layer of neurons is a set of nodes evaluated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2afa906-f659-4cb1-9f77-592435d334b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A list of neurons\n",
    "\n",
    "    nin: number of inputs for the neuron in the layer\n",
    "    nout: how many neurons we will have in a layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0046f322-38d9-4256-b1cc-76a5d82d68da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.7833953133734779),\n",
       " Value(data=0.945892417147608),\n",
       " Value(data=-0.996948563512722)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Layer(2, 3) # a layer of three neurons, with each having two inputs\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3d490-c61f-4f5e-b8a0-dce03cc887a3",
   "metadata": {},
   "source": [
    "Let's complete the picture and define a complete MLP. An MLP, the layers feed into each other sequentially. We take the number of inputs (nins) and list of nouts (sizes of all the layers in the MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a3de073-bb03-4791-a897-ef75c918f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d96a38d9-61c3-46ca-a0ae-37a6e25714aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.6838416421276302)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]   # three inputs into the MLP\n",
    "n = MLP(3, [4, 4, 1])  # 3 layers of size 4, 4, and 1 - the last being the output\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909adbd-f8b0-49f6-8087-073dd57fe4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
